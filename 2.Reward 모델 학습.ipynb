{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11355ac2",
   "metadata": {},
   "source": [
    "# ENV ALL 환경 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8481c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.env import Environment\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7759ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "person_num = 10\n",
    "target_id = None\n",
    "env = Environment(person_num  = person_num, agent_num = 0,step = 100)\n",
    "env.create_person(target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1afd3b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "100%|███████████████████████████████████████| 1000/1000 [02:24<00:00,  6.91it/s]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "actionsss = []\n",
    "statess = []\n",
    "stock_price_list = []\n",
    "rewards = []\n",
    "step_num = 1000\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    env.person_dict[i].agent.model  = load_model('final_model_env/dqn_expert_{}_dense.h5'.format(i+1))\n",
    "\n",
    "\n",
    "for step in tqdm(range(step_num)):\n",
    "    person_action = []\n",
    "\n",
    "    if step == 0: # 첫 주식 배분\n",
    "        for person in env.person_dict.values():\n",
    "            person.buy_first(30)\n",
    "            \n",
    "            \n",
    "    for person,model_id in zip(env.person_dict.values(),range(len(env.person_dict.values()))): # 주문 생성 \n",
    "        action = person.real_invest(state) # 거래소에 주문 진행 (시작시간) / 일반 사람들\n",
    "        person_action.append(action)\n",
    "\n",
    "\n",
    "\n",
    "    env.total_invest.append(env.invest_list)\n",
    "    next_state, reward = env.invest() #거래소 거래 진행 (마감시간)\n",
    "    env.save_person_information()\n",
    "    stock_price_list.append(env.stock_price)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    statess.append(state)\n",
    "    actionsss.append(person_action)\n",
    "    \n",
    "    state = [next_state]\n",
    "\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21de13",
   "metadata": {},
   "source": [
    "# Reward model 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e234b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(values):\n",
    "    min_value = min(values)\n",
    "    max_value = max(values)\n",
    "    \n",
    "    scaled_values = [(value - min_value) / (max_value - min_value) for value in values]\n",
    "    \n",
    "    return scaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c8eb5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4026782450738288,\n",
       " 0.10110430535094739,\n",
       " 0.15724998061650844,\n",
       " 0.40997751514681635,\n",
       " 0.38905441777522143,\n",
       " 0.034934594552687416,\n",
       " 0.0,\n",
       " 0.02875402899770741,\n",
       " 0.5097526666149769,\n",
       " 1.0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaling(rewards[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f56d91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_1 = []\n",
    "data_x_2 = []\n",
    "data_y = []\n",
    "# for state,actions in zip(statess[1:],actionsss[1:]):\n",
    "#     for i in range(person_num):\n",
    "#         test_rate = min_max_scaling(rewards[-1])\n",
    "#         data_x_1.append(np.array(list(state[0][0]),dtype='float32'))\n",
    "#         data_x_2.append(np.array(actions[i],dtype='float32'))\n",
    "#         data_y.append([test_rate[i]])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "scaled_rewards = min_max_scaling(rewards[-1])\n",
    "\n",
    "# 모든 조합에 대해 비교\n",
    "for state, actions in zip(statess[1:], actionsss[1:]):\n",
    "    for i in range(person_num):\n",
    "        for j in range(i+1, person_num):\n",
    "            data_x_1.append(np.array(list(state[0][0]), dtype='float32'))\n",
    "            data_x_2.append(np.array(actions[i], dtype='float32'))\n",
    "            \n",
    "            data_x_1.append(np.array(list(state[0][0]), dtype='float32'))\n",
    "            data_x_2.append(np.array(actions[j], dtype='float32'))\n",
    "            \n",
    "            # test_rate를 기반으로 라벨을 생성\n",
    "            if scaled_rewards[i] > scaled_rewards[j]:\n",
    "                data_y.append([1.0])  # i가 j보다 크면\n",
    "                data_y.append([0.0])\n",
    "            else:\n",
    "                data_y.append([0.0])\n",
    "                data_y.append([1.0])  # j가 i보다 크거나 같으면\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d302610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_train, state_test, action_train, action_test, label_train, label_test = train_test_split(\n",
    "    data_x_1, data_x_2, data_y, test_size=0.3,shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cf85f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 6, 8],\n",
       " [1, 0, 5],\n",
       " [1, 8, 1],\n",
       " [0, 0, 3],\n",
       " [0, 5, 4],\n",
       " [1, 6, 2],\n",
       " [2, 3, 6],\n",
       " [2, 9, 3],\n",
       " [2, 5, 7],\n",
       " [0, 7, 0]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c59e5",
   "metadata": {},
   "source": [
    "# Reward model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d7a5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Concatenate\n",
    "state_dim = 37\n",
    "action_dim = 3\n",
    "\n",
    "\n",
    "\n",
    "# GAIL 모델 생성\n",
    "state_input = Input(shape=(state_dim,), name='state_input')\n",
    "x = Dense(64, activation='relu')(state_input)\n",
    "\n",
    "\n",
    "\n",
    "action_input = Input(shape=(action_dim,), name='action_input')\n",
    "xc = Dense(64, activation='relu')(action_input)\n",
    "\n",
    "\n",
    "\n",
    "input_combined = Concatenate()([x, xc])\n",
    "\n",
    "\n",
    "hidden_layer = Dense(128, activation='relu')(input_combined)\n",
    "hidden_layer = Dense(64, activation='relu')(hidden_layer)\n",
    "output_layer = Dense(1,activation='sigmoid')(hidden_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac79fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Model(inputs=[state_input, action_input], outputs=output_layer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac644124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1859.1119 - accuracy: 0.4649 - val_loss: 69.9806 - val_accuracy: 0.5167\n",
      "Epoch 2/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1017.8832 - accuracy: 0.4835 - val_loss: 641.9701 - val_accuracy: 0.5083\n",
      "Epoch 3/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 518.6537 - accuracy: 0.4857 - val_loss: 195.5346 - val_accuracy: 0.5083\n",
      "Epoch 4/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 226.3432 - accuracy: 0.4843 - val_loss: 109.1136 - val_accuracy: 0.4917\n",
      "Epoch 5/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 460.2268 - accuracy: 0.5043 - val_loss: 970.1797 - val_accuracy: 0.4917\n",
      "Epoch 6/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 324.8187 - accuracy: 0.5000 - val_loss: 12.7056 - val_accuracy: 0.5050\n",
      "Epoch 7/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 402.3956 - accuracy: 0.4886 - val_loss: 950.6907 - val_accuracy: 0.4917\n",
      "Epoch 8/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 223.0078 - accuracy: 0.4979 - val_loss: 54.4210 - val_accuracy: 0.4917\n",
      "Epoch 9/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 230.0456 - accuracy: 0.5000 - val_loss: 389.8114 - val_accuracy: 0.5083\n",
      "Epoch 10/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 346.5757 - accuracy: 0.5021 - val_loss: 104.3401 - val_accuracy: 0.4917\n",
      "Epoch 11/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 239.5463 - accuracy: 0.5079 - val_loss: 405.5448 - val_accuracy: 0.5083\n",
      "Epoch 12/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 309.2230 - accuracy: 0.5036 - val_loss: 246.5567 - val_accuracy: 0.4917\n",
      "Epoch 13/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 90.1691 - accuracy: 0.5100 - val_loss: 121.0291 - val_accuracy: 0.5083\n",
      "Epoch 14/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 318.8609 - accuracy: 0.4950 - val_loss: 389.1029 - val_accuracy: 0.5083\n",
      "Epoch 15/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 351.3103 - accuracy: 0.5093 - val_loss: 187.8812 - val_accuracy: 0.4917\n",
      "Epoch 16/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 167.7490 - accuracy: 0.5229 - val_loss: 22.5565 - val_accuracy: 0.4917\n",
      "Epoch 17/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 107.7770 - accuracy: 0.5050 - val_loss: 258.2313 - val_accuracy: 0.4917\n",
      "Epoch 18/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 108.1702 - accuracy: 0.5222 - val_loss: 109.6845 - val_accuracy: 0.4917\n",
      "Epoch 19/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 192.7873 - accuracy: 0.4964 - val_loss: 74.7179 - val_accuracy: 0.5083\n",
      "Epoch 20/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 105.9287 - accuracy: 0.4993 - val_loss: 157.4250 - val_accuracy: 0.5083\n",
      "Epoch 21/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 157.1065 - accuracy: 0.5064 - val_loss: 131.2677 - val_accuracy: 0.4917\n",
      "Epoch 22/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 62.8637 - accuracy: 0.5651 - val_loss: 167.8637 - val_accuracy: 0.4917\n",
      "Epoch 23/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 148.4924 - accuracy: 0.4950 - val_loss: 114.9901 - val_accuracy: 0.5083\n",
      "Epoch 24/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 197.3229 - accuracy: 0.5086 - val_loss: 172.7612 - val_accuracy: 0.5083\n",
      "Epoch 25/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 166.0806 - accuracy: 0.4957 - val_loss: 16.6829 - val_accuracy: 0.4917\n",
      "Epoch 26/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 200.7413 - accuracy: 0.4893 - val_loss: 9.8352 - val_accuracy: 0.5417\n",
      "Epoch 27/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 86.3263 - accuracy: 0.5029 - val_loss: 79.1639 - val_accuracy: 0.5083\n",
      "Epoch 28/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 95.0642 - accuracy: 0.5093 - val_loss: 115.1598 - val_accuracy: 0.4917\n",
      "Epoch 29/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 192.0962 - accuracy: 0.5129 - val_loss: 185.8081 - val_accuracy: 0.4917\n",
      "Epoch 30/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 106.0786 - accuracy: 0.5122 - val_loss: 80.1423 - val_accuracy: 0.4917\n",
      "Epoch 31/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 164.4123 - accuracy: 0.5215 - val_loss: 29.4541 - val_accuracy: 0.4917\n",
      "Epoch 32/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 70.9332 - accuracy: 0.5093 - val_loss: 163.9694 - val_accuracy: 0.5083\n",
      "Epoch 33/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 96.1535 - accuracy: 0.4921 - val_loss: 83.8083 - val_accuracy: 0.4917\n",
      "Epoch 34/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 85.4010 - accuracy: 0.5386 - val_loss: 75.9006 - val_accuracy: 0.4917\n",
      "Epoch 35/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 31.8909 - accuracy: 0.5522 - val_loss: 16.9214 - val_accuracy: 0.4950\n",
      "Epoch 36/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 77.0495 - accuracy: 0.5279 - val_loss: 194.6032 - val_accuracy: 0.5083\n",
      "Epoch 37/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 161.3599 - accuracy: 0.5236 - val_loss: 21.0155 - val_accuracy: 0.5100\n",
      "Epoch 38/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 91.1401 - accuracy: 0.4986 - val_loss: 154.9802 - val_accuracy: 0.4917\n",
      "Epoch 39/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 130.9043 - accuracy: 0.4907 - val_loss: 84.1261 - val_accuracy: 0.5083\n",
      "Epoch 40/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 86.6003 - accuracy: 0.5329 - val_loss: 28.3561 - val_accuracy: 0.4917\n",
      "Epoch 41/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 100.9212 - accuracy: 0.4971 - val_loss: 186.5600 - val_accuracy: 0.4917\n",
      "Epoch 42/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 143.7358 - accuracy: 0.4914 - val_loss: 59.6126 - val_accuracy: 0.4917\n",
      "Epoch 43/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 60.9328 - accuracy: 0.5329 - val_loss: 187.9463 - val_accuracy: 0.5083\n",
      "Epoch 44/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 119.4656 - accuracy: 0.5079 - val_loss: 39.2545 - val_accuracy: 0.4917\n",
      "Epoch 45/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 58.4859 - accuracy: 0.5279 - val_loss: 10.9522 - val_accuracy: 0.6217\n",
      "Epoch 46/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 48.5116 - accuracy: 0.5594 - val_loss: 130.0138 - val_accuracy: 0.5083\n",
      "Epoch 47/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 80.6107 - accuracy: 0.5265 - val_loss: 39.2766 - val_accuracy: 0.4917\n",
      "Epoch 48/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 154.7111 - accuracy: 0.5114 - val_loss: 338.0367 - val_accuracy: 0.4917\n",
      "Epoch 49/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 100.2439 - accuracy: 0.5415 - val_loss: 35.0566 - val_accuracy: 0.5083\n",
      "Epoch 50/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 88.1626 - accuracy: 0.5215 - val_loss: 36.5755 - val_accuracy: 0.5083\n",
      "Epoch 51/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 47.6253 - accuracy: 0.5336 - val_loss: 35.6102 - val_accuracy: 0.5083\n",
      "Epoch 52/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 19.2517 - accuracy: 0.6109 - val_loss: 37.9238 - val_accuracy: 0.4917\n",
      "Epoch 53/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 14.7965 - accuracy: 0.6309 - val_loss: 29.7048 - val_accuracy: 0.5083\n",
      "Epoch 54/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 18.7869 - accuracy: 0.6016 - val_loss: 6.5180 - val_accuracy: 0.6967\n",
      "Epoch 55/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 54.7356 - accuracy: 0.5236 - val_loss: 46.2411 - val_accuracy: 0.4917\n",
      "Epoch 56/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 36.8836 - accuracy: 0.5129 - val_loss: 43.6094 - val_accuracy: 0.5083\n",
      "Epoch 57/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 14.3850 - accuracy: 0.6810 - val_loss: 17.5914 - val_accuracy: 0.5683\n",
      "Epoch 58/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.2998 - accuracy: 0.6252 - val_loss: 9.0413 - val_accuracy: 0.6467\n",
      "Epoch 59/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 35.8325 - accuracy: 0.5815 - val_loss: 5.9872 - val_accuracy: 0.7067\n",
      "Epoch 60/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 62.1677 - accuracy: 0.5272 - val_loss: 21.7288 - val_accuracy: 0.5383\n",
      "Epoch 61/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 28.6323 - accuracy: 0.5479 - val_loss: 33.9968 - val_accuracy: 0.4917\n",
      "Epoch 62/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 33.5173 - accuracy: 0.5322 - val_loss: 40.5095 - val_accuracy: 0.5083\n",
      "Epoch 63/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 37.8080 - accuracy: 0.5043 - val_loss: 26.7056 - val_accuracy: 0.5233\n",
      "Epoch 64/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 24.8371 - accuracy: 0.5815 - val_loss: 46.8501 - val_accuracy: 0.5083\n",
      "Epoch 65/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 38.2460 - accuracy: 0.5708 - val_loss: 97.7941 - val_accuracy: 0.4917\n",
      "Epoch 66/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 70.6013 - accuracy: 0.5300 - val_loss: 53.2600 - val_accuracy: 0.5083\n",
      "Epoch 67/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 48.3110 - accuracy: 0.5451 - val_loss: 6.6897 - val_accuracy: 0.7083\n",
      "Epoch 68/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 19.0275 - accuracy: 0.6180 - val_loss: 11.5281 - val_accuracy: 0.6433\n",
      "Epoch 69/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.3803 - accuracy: 0.6502 - val_loss: 6.4254 - val_accuracy: 0.7217\n",
      "Epoch 70/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.1768 - accuracy: 0.6853 - val_loss: 11.3942 - val_accuracy: 0.6650\n",
      "Epoch 71/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 10.5191 - accuracy: 0.6760 - val_loss: 14.0323 - val_accuracy: 0.6350\n",
      "Epoch 72/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 17.2299 - accuracy: 0.6273 - val_loss: 8.7776 - val_accuracy: 0.6983\n",
      "Epoch 73/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 17.6211 - accuracy: 0.6330 - val_loss: 41.4090 - val_accuracy: 0.4917\n",
      "Epoch 74/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 50.4653 - accuracy: 0.5594 - val_loss: 64.5941 - val_accuracy: 0.4917\n",
      "Epoch 75/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 29.7238 - accuracy: 0.5408 - val_loss: 6.2902 - val_accuracy: 0.7300\n",
      "Epoch 76/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.3788 - accuracy: 0.6738 - val_loss: 8.9235 - val_accuracy: 0.6650\n",
      "Epoch 77/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 34.3130 - accuracy: 0.5229 - val_loss: 12.7521 - val_accuracy: 0.6667\n",
      "Epoch 78/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 23.1539 - accuracy: 0.6030 - val_loss: 35.3115 - val_accuracy: 0.4917\n",
      "Epoch 79/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.0938 - accuracy: 0.6588 - val_loss: 9.9180 - val_accuracy: 0.7050\n",
      "Epoch 80/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 10.3888 - accuracy: 0.6724 - val_loss: 12.2555 - val_accuracy: 0.6717\n",
      "Epoch 81/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.6710 - accuracy: 0.6860 - val_loss: 6.1057 - val_accuracy: 0.7250\n",
      "Epoch 82/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.8620 - accuracy: 0.6881 - val_loss: 6.1288 - val_accuracy: 0.7367\n",
      "Epoch 83/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 22.3811 - accuracy: 0.5908 - val_loss: 5.9501 - val_accuracy: 0.7400\n",
      "Epoch 84/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 53.8103 - accuracy: 0.5179 - val_loss: 7.4034 - val_accuracy: 0.7017\n",
      "Epoch 85/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 29.5697 - accuracy: 0.5601 - val_loss: 58.0098 - val_accuracy: 0.4917\n",
      "Epoch 86/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 35.3271 - accuracy: 0.5529 - val_loss: 26.8050 - val_accuracy: 0.5117\n",
      "Epoch 87/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 28.7115 - accuracy: 0.5622 - val_loss: 8.8039 - val_accuracy: 0.7100\n",
      "Epoch 88/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 15.2630 - accuracy: 0.6402 - val_loss: 6.3848 - val_accuracy: 0.7233\n",
      "Epoch 89/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 23.4191 - accuracy: 0.5980 - val_loss: 92.9651 - val_accuracy: 0.5083\n",
      "Epoch 90/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 118.3426 - accuracy: 0.5007 - val_loss: 20.1768 - val_accuracy: 0.5617\n",
      "Epoch 91/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 74.0059 - accuracy: 0.4807 - val_loss: 33.3285 - val_accuracy: 0.4933\n",
      "Epoch 92/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 36.2577 - accuracy: 0.5629 - val_loss: 24.4926 - val_accuracy: 0.5433\n",
      "Epoch 93/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 18.4468 - accuracy: 0.6030 - val_loss: 9.7170 - val_accuracy: 0.7100\n",
      "Epoch 94/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 15.2620 - accuracy: 0.6502 - val_loss: 7.2028 - val_accuracy: 0.7133\n",
      "Epoch 95/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 20.9354 - accuracy: 0.5966 - val_loss: 7.4022 - val_accuracy: 0.7017\n",
      "Epoch 96/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 19.4314 - accuracy: 0.6052 - val_loss: 32.3088 - val_accuracy: 0.5200\n",
      "Epoch 97/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 14.9538 - accuracy: 0.6667 - val_loss: 6.8906 - val_accuracy: 0.7017\n",
      "Epoch 98/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.1936 - accuracy: 0.6953 - val_loss: 8.0390 - val_accuracy: 0.7067\n",
      "Epoch 99/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.1274 - accuracy: 0.6924 - val_loss: 7.0451 - val_accuracy: 0.6950\n",
      "Epoch 100/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.6477 - accuracy: 0.7031 - val_loss: 13.6104 - val_accuracy: 0.6317\n",
      "Epoch 101/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 15.3473 - accuracy: 0.6180 - val_loss: 6.1527 - val_accuracy: 0.7183\n",
      "Epoch 102/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 17.1229 - accuracy: 0.6202 - val_loss: 21.3443 - val_accuracy: 0.5800\n",
      "Epoch 103/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 20.9392 - accuracy: 0.6001 - val_loss: 30.4137 - val_accuracy: 0.5200\n",
      "Epoch 104/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 10.4382 - accuracy: 0.6845 - val_loss: 6.6819 - val_accuracy: 0.7283\n",
      "Epoch 105/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 11.3928 - accuracy: 0.6702 - val_loss: 7.6076 - val_accuracy: 0.7083\n",
      "Epoch 106/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 24.1500 - accuracy: 0.5694 - val_loss: 43.0614 - val_accuracy: 0.5083\n",
      "Epoch 107/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 16.2163 - accuracy: 0.6423 - val_loss: 13.9632 - val_accuracy: 0.6667\n",
      "Epoch 108/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.6198 - accuracy: 0.7103 - val_loss: 9.3273 - val_accuracy: 0.7233\n",
      "Epoch 109/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.8854 - accuracy: 0.6917 - val_loss: 7.7154 - val_accuracy: 0.6750\n",
      "Epoch 110/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.9039 - accuracy: 0.7239 - val_loss: 14.0679 - val_accuracy: 0.6533\n",
      "Epoch 111/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 10.8823 - accuracy: 0.6724 - val_loss: 9.3127 - val_accuracy: 0.7200\n",
      "Epoch 112/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.7826 - accuracy: 0.7024 - val_loss: 24.7472 - val_accuracy: 0.5300\n",
      "Epoch 113/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.1978 - accuracy: 0.7067 - val_loss: 6.2367 - val_accuracy: 0.7100\n",
      "Epoch 114/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 7.4115 - accuracy: 0.7232 - val_loss: 18.4254 - val_accuracy: 0.5550\n",
      "Epoch 115/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 28.1817 - accuracy: 0.5680 - val_loss: 68.3494 - val_accuracy: 0.5083\n",
      "Epoch 116/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 47.5341 - accuracy: 0.5451 - val_loss: 46.7244 - val_accuracy: 0.5083\n",
      "Epoch 117/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 26.5556 - accuracy: 0.5672 - val_loss: 9.0497 - val_accuracy: 0.6533\n",
      "Epoch 118/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 15.9219 - accuracy: 0.6245 - val_loss: 36.8545 - val_accuracy: 0.4917\n",
      "Epoch 119/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 21.9937 - accuracy: 0.5973 - val_loss: 29.2605 - val_accuracy: 0.4917\n",
      "Epoch 120/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.9541 - accuracy: 0.6974 - val_loss: 5.8275 - val_accuracy: 0.7450\n",
      "Epoch 121/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.2455 - accuracy: 0.7096 - val_loss: 5.9452 - val_accuracy: 0.7383\n",
      "Epoch 122/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 14.3817 - accuracy: 0.6495 - val_loss: 23.7539 - val_accuracy: 0.5533\n",
      "Epoch 123/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.8902 - accuracy: 0.6838 - val_loss: 6.2363 - val_accuracy: 0.7333\n",
      "Epoch 124/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 15.3656 - accuracy: 0.6423 - val_loss: 54.9898 - val_accuracy: 0.5083\n",
      "Epoch 125/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 86.5252 - accuracy: 0.5172 - val_loss: 22.8259 - val_accuracy: 0.5617\n",
      "Epoch 126/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 30.0980 - accuracy: 0.5694 - val_loss: 11.9184 - val_accuracy: 0.6900\n",
      "Epoch 127/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.3409 - accuracy: 0.7082 - val_loss: 8.5151 - val_accuracy: 0.6667\n",
      "Epoch 128/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.4252 - accuracy: 0.6531 - val_loss: 6.6721 - val_accuracy: 0.6983\n",
      "Epoch 129/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 10.3702 - accuracy: 0.6803 - val_loss: 8.9688 - val_accuracy: 0.7183\n",
      "Epoch 130/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.5331 - accuracy: 0.7175 - val_loss: 6.2152 - val_accuracy: 0.7050\n",
      "Epoch 131/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.4285 - accuracy: 0.7275 - val_loss: 5.7215 - val_accuracy: 0.7383\n",
      "Epoch 132/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.4516 - accuracy: 0.6903 - val_loss: 5.5782 - val_accuracy: 0.7167\n",
      "Epoch 133/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.8248 - accuracy: 0.6917 - val_loss: 8.1371 - val_accuracy: 0.7300\n",
      "Epoch 134/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.3831 - accuracy: 0.7024 - val_loss: 6.4955 - val_accuracy: 0.7350\n",
      "Epoch 135/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.5443 - accuracy: 0.7318 - val_loss: 5.6020 - val_accuracy: 0.7333\n",
      "Epoch 136/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 28.5820 - accuracy: 0.5687 - val_loss: 59.0020 - val_accuracy: 0.5083\n",
      "Epoch 137/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 21.1898 - accuracy: 0.5908 - val_loss: 9.8792 - val_accuracy: 0.6450\n",
      "Epoch 138/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 17.4179 - accuracy: 0.6137 - val_loss: 6.2867 - val_accuracy: 0.7317\n",
      "Epoch 139/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.0497 - accuracy: 0.7039 - val_loss: 5.3669 - val_accuracy: 0.7233\n",
      "Epoch 140/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.8940 - accuracy: 0.7039 - val_loss: 14.5154 - val_accuracy: 0.6267\n",
      "Epoch 141/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 11.6739 - accuracy: 0.6345 - val_loss: 5.5046 - val_accuracy: 0.7400\n",
      "Epoch 142/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 16.0833 - accuracy: 0.6266 - val_loss: 9.8115 - val_accuracy: 0.7017\n",
      "Epoch 143/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 22.0517 - accuracy: 0.5923 - val_loss: 5.6597 - val_accuracy: 0.7400\n",
      "Epoch 144/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 11.4034 - accuracy: 0.6567 - val_loss: 8.1811 - val_accuracy: 0.6617\n",
      "Epoch 145/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 22.1976 - accuracy: 0.6009 - val_loss: 54.4668 - val_accuracy: 0.5083\n",
      "Epoch 146/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 62.8930 - accuracy: 0.5172 - val_loss: 6.0183 - val_accuracy: 0.7300\n",
      "Epoch 147/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 45.4043 - accuracy: 0.5172 - val_loss: 40.8190 - val_accuracy: 0.5083\n",
      "Epoch 148/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 21.9989 - accuracy: 0.5787 - val_loss: 15.5867 - val_accuracy: 0.5583\n",
      "Epoch 149/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 14.2250 - accuracy: 0.6352 - val_loss: 5.7719 - val_accuracy: 0.7167\n",
      "Epoch 150/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 13.8142 - accuracy: 0.6359 - val_loss: 12.2167 - val_accuracy: 0.6783\n",
      "Epoch 151/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.7927 - accuracy: 0.7060 - val_loss: 6.3124 - val_accuracy: 0.6983\n",
      "Epoch 152/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.3784 - accuracy: 0.7260 - val_loss: 5.9739 - val_accuracy: 0.7283\n",
      "Epoch 153/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 11.0393 - accuracy: 0.6602 - val_loss: 5.4840 - val_accuracy: 0.7200\n",
      "Epoch 154/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.5522 - accuracy: 0.6924 - val_loss: 5.2862 - val_accuracy: 0.7250\n",
      "Epoch 155/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.3186 - accuracy: 0.7246 - val_loss: 5.5978 - val_accuracy: 0.7183\n",
      "Epoch 156/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.3128 - accuracy: 0.6781 - val_loss: 5.8576 - val_accuracy: 0.6983\n",
      "Epoch 157/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.4841 - accuracy: 0.6609 - val_loss: 6.1197 - val_accuracy: 0.6917\n",
      "Epoch 158/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.1824 - accuracy: 0.7074 - val_loss: 5.4020 - val_accuracy: 0.7400\n",
      "Epoch 159/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.2707 - accuracy: 0.7132 - val_loss: 6.1884 - val_accuracy: 0.6883\n",
      "Epoch 160/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.1667 - accuracy: 0.7210 - val_loss: 11.6513 - val_accuracy: 0.5883\n",
      "Epoch 161/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 15.0481 - accuracy: 0.6059 - val_loss: 11.9589 - val_accuracy: 0.6767\n",
      "Epoch 162/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 16.5677 - accuracy: 0.6159 - val_loss: 5.2477 - val_accuracy: 0.7467\n",
      "Epoch 163/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.1087 - accuracy: 0.7353 - val_loss: 6.0850 - val_accuracy: 0.7533\n",
      "Epoch 164/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 26.6728 - accuracy: 0.5687 - val_loss: 5.2788 - val_accuracy: 0.7417\n",
      "Epoch 165/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.3461 - accuracy: 0.6745 - val_loss: 5.0493 - val_accuracy: 0.7467\n",
      "Epoch 166/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.5424 - accuracy: 0.6981 - val_loss: 7.1246 - val_accuracy: 0.7433\n",
      "Epoch 167/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.0729 - accuracy: 0.7110 - val_loss: 9.1483 - val_accuracy: 0.7050\n",
      "Epoch 168/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.1097 - accuracy: 0.6710 - val_loss: 9.0152 - val_accuracy: 0.7083\n",
      "Epoch 169/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.2487 - accuracy: 0.7053 - val_loss: 4.9697 - val_accuracy: 0.7500\n",
      "Epoch 170/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.0474 - accuracy: 0.7389 - val_loss: 4.8763 - val_accuracy: 0.7500\n",
      "Epoch 171/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 4.9717 - accuracy: 0.7253 - val_loss: 5.5875 - val_accuracy: 0.6883\n",
      "Epoch 172/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.3562 - accuracy: 0.6810 - val_loss: 8.0300 - val_accuracy: 0.7133\n",
      "Epoch 173/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.5258 - accuracy: 0.6774 - val_loss: 8.4718 - val_accuracy: 0.6350\n",
      "Epoch 174/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.4336 - accuracy: 0.7046 - val_loss: 4.6182 - val_accuracy: 0.7500\n",
      "Epoch 175/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.4088 - accuracy: 0.7246 - val_loss: 4.8332 - val_accuracy: 0.7400\n",
      "Epoch 176/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.9655 - accuracy: 0.6574 - val_loss: 4.8345 - val_accuracy: 0.7067\n",
      "Epoch 177/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.6868 - accuracy: 0.6867 - val_loss: 9.6837 - val_accuracy: 0.6867\n",
      "Epoch 178/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.9749 - accuracy: 0.6667 - val_loss: 11.5139 - val_accuracy: 0.6433\n",
      "Epoch 179/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 10.9558 - accuracy: 0.6316 - val_loss: 15.8348 - val_accuracy: 0.5750\n",
      "Epoch 180/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.2632 - accuracy: 0.6767 - val_loss: 4.5867 - val_accuracy: 0.7133\n",
      "Epoch 181/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 27.5688 - accuracy: 0.5594 - val_loss: 89.8305 - val_accuracy: 0.4917\n",
      "Epoch 182/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 39.6804 - accuracy: 0.5265 - val_loss: 9.9348 - val_accuracy: 0.5833\n",
      "Epoch 183/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 18.1025 - accuracy: 0.5687 - val_loss: 6.0782 - val_accuracy: 0.6650\n",
      "Epoch 184/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.9771 - accuracy: 0.6774 - val_loss: 12.4583 - val_accuracy: 0.5583\n",
      "Epoch 185/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.5254 - accuracy: 0.6824 - val_loss: 8.5695 - val_accuracy: 0.7033\n",
      "Epoch 186/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.7686 - accuracy: 0.6960 - val_loss: 7.3676 - val_accuracy: 0.6533\n",
      "Epoch 187/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.5592 - accuracy: 0.6352 - val_loss: 5.7461 - val_accuracy: 0.7750\n",
      "Epoch 188/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.5714 - accuracy: 0.6803 - val_loss: 4.4844 - val_accuracy: 0.7417\n",
      "Epoch 189/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.4819 - accuracy: 0.6917 - val_loss: 5.0487 - val_accuracy: 0.6933\n",
      "Epoch 190/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.4537 - accuracy: 0.7074 - val_loss: 7.4594 - val_accuracy: 0.6417\n",
      "Epoch 191/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.6770 - accuracy: 0.6810 - val_loss: 17.5471 - val_accuracy: 0.5217\n",
      "Epoch 192/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.9366 - accuracy: 0.7053 - val_loss: 4.6284 - val_accuracy: 0.7117\n",
      "Epoch 193/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.2156 - accuracy: 0.7303 - val_loss: 15.5995 - val_accuracy: 0.5650\n",
      "Epoch 194/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 13.5944 - accuracy: 0.5930 - val_loss: 4.8184 - val_accuracy: 0.7483\n",
      "Epoch 195/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.6128 - accuracy: 0.6624 - val_loss: 7.6242 - val_accuracy: 0.7133\n",
      "Epoch 196/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.3935 - accuracy: 0.7074 - val_loss: 12.7383 - val_accuracy: 0.5517\n",
      "Epoch 197/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.6691 - accuracy: 0.6216 - val_loss: 30.3449 - val_accuracy: 0.5083\n",
      "Epoch 198/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.0510 - accuracy: 0.6853 - val_loss: 4.3658 - val_accuracy: 0.7467\n",
      "Epoch 199/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.2977 - accuracy: 0.7117 - val_loss: 5.3467 - val_accuracy: 0.7783\n",
      "Epoch 200/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.1065 - accuracy: 0.6881 - val_loss: 6.9034 - val_accuracy: 0.7217\n",
      "Epoch 201/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 15.6815 - accuracy: 0.5665 - val_loss: 36.8534 - val_accuracy: 0.4917\n",
      "Epoch 202/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 11.3630 - accuracy: 0.6502 - val_loss: 5.6325 - val_accuracy: 0.6567\n",
      "Epoch 203/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.0245 - accuracy: 0.6495 - val_loss: 4.4780 - val_accuracy: 0.7433\n",
      "Epoch 204/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.9498 - accuracy: 0.6974 - val_loss: 4.3267 - val_accuracy: 0.7100\n",
      "Epoch 205/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.4875 - accuracy: 0.7246 - val_loss: 4.4625 - val_accuracy: 0.7017\n",
      "Epoch 206/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.8687 - accuracy: 0.7210 - val_loss: 4.4962 - val_accuracy: 0.7467\n",
      "Epoch 207/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.0272 - accuracy: 0.7232 - val_loss: 6.0190 - val_accuracy: 0.6517\n",
      "Epoch 208/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.9182 - accuracy: 0.7268 - val_loss: 5.3344 - val_accuracy: 0.6583\n",
      "Epoch 209/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.9522 - accuracy: 0.6946 - val_loss: 3.8979 - val_accuracy: 0.7500\n",
      "Epoch 210/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.3539 - accuracy: 0.7225 - val_loss: 5.6857 - val_accuracy: 0.6467\n",
      "Epoch 211/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.7824 - accuracy: 0.6602 - val_loss: 5.7412 - val_accuracy: 0.7517\n",
      "Epoch 212/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.5658 - accuracy: 0.6867 - val_loss: 4.2241 - val_accuracy: 0.7483\n",
      "Epoch 213/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.0353 - accuracy: 0.6831 - val_loss: 5.2087 - val_accuracy: 0.6567\n",
      "Epoch 214/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.0905 - accuracy: 0.7132 - val_loss: 3.9636 - val_accuracy: 0.7050\n",
      "Epoch 215/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.9866 - accuracy: 0.7096 - val_loss: 5.2383 - val_accuracy: 0.6483\n",
      "Epoch 216/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.4009 - accuracy: 0.7053 - val_loss: 3.9895 - val_accuracy: 0.7517\n",
      "Epoch 217/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.8534 - accuracy: 0.7139 - val_loss: 11.7641 - val_accuracy: 0.5883\n",
      "Epoch 218/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.2736 - accuracy: 0.6438 - val_loss: 8.1559 - val_accuracy: 0.6850\n",
      "Epoch 219/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.4622 - accuracy: 0.6788 - val_loss: 7.0850 - val_accuracy: 0.6983\n",
      "Epoch 220/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.9256 - accuracy: 0.7003 - val_loss: 8.4339 - val_accuracy: 0.6733\n",
      "Epoch 221/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 10.4206 - accuracy: 0.6080 - val_loss: 3.4505 - val_accuracy: 0.7450\n",
      "Epoch 222/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.6740 - accuracy: 0.6152 - val_loss: 41.1272 - val_accuracy: 0.5083\n",
      "Epoch 223/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 15.8259 - accuracy: 0.6037 - val_loss: 3.5416 - val_accuracy: 0.7567\n",
      "Epoch 224/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.0383 - accuracy: 0.6781 - val_loss: 8.4313 - val_accuracy: 0.6750\n",
      "Epoch 225/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 11.1204 - accuracy: 0.5858 - val_loss: 13.0738 - val_accuracy: 0.5383\n",
      "Epoch 226/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.1498 - accuracy: 0.6745 - val_loss: 6.2653 - val_accuracy: 0.6333\n",
      "Epoch 227/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.4409 - accuracy: 0.7124 - val_loss: 3.7076 - val_accuracy: 0.7533\n",
      "Epoch 228/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 4.9230 - accuracy: 0.6981 - val_loss: 3.5725 - val_accuracy: 0.7150\n",
      "Epoch 229/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.0475 - accuracy: 0.6252 - val_loss: 5.1092 - val_accuracy: 0.7567\n",
      "Epoch 230/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 10.5678 - accuracy: 0.6066 - val_loss: 12.6918 - val_accuracy: 0.5683\n",
      "Epoch 231/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.2886 - accuracy: 0.6359 - val_loss: 4.3854 - val_accuracy: 0.6783\n",
      "Epoch 232/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.5123 - accuracy: 0.6459 - val_loss: 7.6788 - val_accuracy: 0.5833\n",
      "Epoch 233/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.4611 - accuracy: 0.6652 - val_loss: 6.5550 - val_accuracy: 0.6917\n",
      "Epoch 234/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.9931 - accuracy: 0.6602 - val_loss: 3.3901 - val_accuracy: 0.7500\n",
      "Epoch 235/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.9045 - accuracy: 0.7182 - val_loss: 3.1040 - val_accuracy: 0.7433\n",
      "Epoch 236/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 15.8992 - accuracy: 0.6323 - val_loss: 15.8243 - val_accuracy: 0.4917\n",
      "Epoch 237/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 22.1647 - accuracy: 0.5401 - val_loss: 7.3551 - val_accuracy: 0.6750\n",
      "Epoch 238/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.8541 - accuracy: 0.7310 - val_loss: 3.5334 - val_accuracy: 0.7467\n",
      "Epoch 239/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.5165 - accuracy: 0.7117 - val_loss: 5.7515 - val_accuracy: 0.7167\n",
      "Epoch 240/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.7363 - accuracy: 0.7153 - val_loss: 6.4598 - val_accuracy: 0.6933\n",
      "Epoch 241/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.0821 - accuracy: 0.6516 - val_loss: 5.4317 - val_accuracy: 0.7183\n",
      "Epoch 242/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.4813 - accuracy: 0.7132 - val_loss: 4.4847 - val_accuracy: 0.7683\n",
      "Epoch 243/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.3154 - accuracy: 0.6838 - val_loss: 6.7013 - val_accuracy: 0.6900\n",
      "Epoch 244/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.3764 - accuracy: 0.6896 - val_loss: 12.9555 - val_accuracy: 0.5283\n",
      "Epoch 245/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.6729 - accuracy: 0.6588 - val_loss: 7.6901 - val_accuracy: 0.5600\n",
      "Epoch 246/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.1968 - accuracy: 0.6996 - val_loss: 3.0311 - val_accuracy: 0.7183\n",
      "Epoch 247/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.1146 - accuracy: 0.6960 - val_loss: 10.2399 - val_accuracy: 0.5833\n",
      "Epoch 248/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.4366 - accuracy: 0.6323 - val_loss: 7.0718 - val_accuracy: 0.6767\n",
      "Epoch 249/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.0813 - accuracy: 0.6953 - val_loss: 3.4455 - val_accuracy: 0.6900\n",
      "Epoch 250/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.9127 - accuracy: 0.6195 - val_loss: 3.8312 - val_accuracy: 0.7783\n",
      "Epoch 251/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.9510 - accuracy: 0.6745 - val_loss: 3.1662 - val_accuracy: 0.7533\n",
      "Epoch 252/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.1611 - accuracy: 0.7124 - val_loss: 3.8966 - val_accuracy: 0.6700\n",
      "Epoch 253/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.4837 - accuracy: 0.7253 - val_loss: 4.2227 - val_accuracy: 0.7717\n",
      "Epoch 254/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.6198 - accuracy: 0.7160 - val_loss: 3.2291 - val_accuracy: 0.6917\n",
      "Epoch 255/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.8110 - accuracy: 0.7217 - val_loss: 2.8592 - val_accuracy: 0.7550\n",
      "Epoch 256/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.9874 - accuracy: 0.6373 - val_loss: 3.9186 - val_accuracy: 0.6667\n",
      "Epoch 257/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.7642 - accuracy: 0.7046 - val_loss: 3.7322 - val_accuracy: 0.7733\n",
      "Epoch 258/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.8396 - accuracy: 0.7003 - val_loss: 3.3252 - val_accuracy: 0.7750\n",
      "Epoch 259/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.7250 - accuracy: 0.6631 - val_loss: 19.1876 - val_accuracy: 0.5083\n",
      "Epoch 260/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 16.4256 - accuracy: 0.5465 - val_loss: 3.7666 - val_accuracy: 0.6650\n",
      "Epoch 261/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.1632 - accuracy: 0.6309 - val_loss: 8.2601 - val_accuracy: 0.6183\n",
      "Epoch 262/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.8575 - accuracy: 0.6888 - val_loss: 2.9001 - val_accuracy: 0.7450\n",
      "Epoch 263/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.8195 - accuracy: 0.7310 - val_loss: 5.5921 - val_accuracy: 0.6083\n",
      "Epoch 264/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.5971 - accuracy: 0.7182 - val_loss: 3.6178 - val_accuracy: 0.7800\n",
      "Epoch 265/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.7110 - accuracy: 0.7189 - val_loss: 3.5043 - val_accuracy: 0.6583\n",
      "Epoch 266/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.9664 - accuracy: 0.6753 - val_loss: 2.8125 - val_accuracy: 0.7633\n",
      "Epoch 267/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.3503 - accuracy: 0.6259 - val_loss: 7.1010 - val_accuracy: 0.5583\n",
      "Epoch 268/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.1112 - accuracy: 0.6838 - val_loss: 8.0528 - val_accuracy: 0.6000\n",
      "Epoch 269/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.1917 - accuracy: 0.6953 - val_loss: 3.6408 - val_accuracy: 0.7700\n",
      "Epoch 270/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.9569 - accuracy: 0.7332 - val_loss: 3.2249 - val_accuracy: 0.7917\n",
      "Epoch 271/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.0662 - accuracy: 0.7046 - val_loss: 2.6083 - val_accuracy: 0.7567\n",
      "Epoch 272/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.8281 - accuracy: 0.7325 - val_loss: 2.4668 - val_accuracy: 0.7450\n",
      "Epoch 273/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.1268 - accuracy: 0.7203 - val_loss: 2.6168 - val_accuracy: 0.7517\n",
      "Epoch 274/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.1134 - accuracy: 0.6874 - val_loss: 2.9200 - val_accuracy: 0.7800\n",
      "Epoch 275/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.2346 - accuracy: 0.7210 - val_loss: 2.7994 - val_accuracy: 0.7833\n",
      "Epoch 276/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.4166 - accuracy: 0.7124 - val_loss: 2.3522 - val_accuracy: 0.7167\n",
      "Epoch 277/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.6678 - accuracy: 0.7139 - val_loss: 2.6639 - val_accuracy: 0.6900\n",
      "Epoch 278/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 21.4618 - accuracy: 0.5565 - val_loss: 7.4702 - val_accuracy: 0.5217\n",
      "Epoch 279/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 13.3832 - accuracy: 0.5866 - val_loss: 15.8712 - val_accuracy: 0.5083\n",
      "Epoch 280/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.4558 - accuracy: 0.6509 - val_loss: 2.8668 - val_accuracy: 0.7733\n",
      "Epoch 281/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.3864 - accuracy: 0.6381 - val_loss: 2.3834 - val_accuracy: 0.7550\n",
      "Epoch 282/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.4800 - accuracy: 0.6395 - val_loss: 4.0372 - val_accuracy: 0.6450\n",
      "Epoch 283/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 11.4179 - accuracy: 0.5622 - val_loss: 17.9212 - val_accuracy: 0.5083\n",
      "Epoch 284/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.1106 - accuracy: 0.6559 - val_loss: 3.5477 - val_accuracy: 0.7533\n",
      "Epoch 285/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 3.3965 - accuracy: 0.7182 - val_loss: 2.2838 - val_accuracy: 0.7500\n",
      "Epoch 286/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.1109 - accuracy: 0.7067 - val_loss: 9.9565 - val_accuracy: 0.4983\n",
      "Epoch 287/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.7664 - accuracy: 0.5837 - val_loss: 12.4130 - val_accuracy: 0.5083\n",
      "Epoch 288/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.1035 - accuracy: 0.6931 - val_loss: 2.3543 - val_accuracy: 0.7083\n",
      "Epoch 289/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.8422 - accuracy: 0.7210 - val_loss: 2.2448 - val_accuracy: 0.7367\n",
      "Epoch 290/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.1990 - accuracy: 0.6488 - val_loss: 3.4815 - val_accuracy: 0.6417\n",
      "Epoch 291/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.5316 - accuracy: 0.6788 - val_loss: 2.7136 - val_accuracy: 0.7817\n",
      "Epoch 292/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.8687 - accuracy: 0.7239 - val_loss: 3.2894 - val_accuracy: 0.6433\n",
      "Epoch 293/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.0536 - accuracy: 0.6788 - val_loss: 10.1534 - val_accuracy: 0.4933\n",
      "Epoch 294/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 12.4369 - accuracy: 0.5851 - val_loss: 3.3812 - val_accuracy: 0.7567\n",
      "Epoch 295/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.9240 - accuracy: 0.6245 - val_loss: 4.6569 - val_accuracy: 0.6017\n",
      "Epoch 296/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.2353 - accuracy: 0.6009 - val_loss: 2.3440 - val_accuracy: 0.7033\n",
      "Epoch 297/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.9396 - accuracy: 0.7225 - val_loss: 7.2803 - val_accuracy: 0.5450\n",
      "Epoch 298/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.7498 - accuracy: 0.6509 - val_loss: 10.3346 - val_accuracy: 0.5100\n",
      "Epoch 299/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.5296 - accuracy: 0.6581 - val_loss: 2.3922 - val_accuracy: 0.7067\n",
      "Epoch 300/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.0711 - accuracy: 0.7189 - val_loss: 3.6488 - val_accuracy: 0.7300\n",
      "Epoch 301/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.6015 - accuracy: 0.7160 - val_loss: 7.4979 - val_accuracy: 0.5433\n",
      "Epoch 302/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.3863 - accuracy: 0.6731 - val_loss: 2.0968 - val_accuracy: 0.7517\n",
      "Epoch 303/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.0473 - accuracy: 0.7067 - val_loss: 2.0580 - val_accuracy: 0.7250\n",
      "Epoch 304/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.3454 - accuracy: 0.6917 - val_loss: 2.1982 - val_accuracy: 0.7050\n",
      "Epoch 305/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.4907 - accuracy: 0.7260 - val_loss: 2.4247 - val_accuracy: 0.7833\n",
      "Epoch 306/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.1695 - accuracy: 0.7439 - val_loss: 2.9984 - val_accuracy: 0.7633\n",
      "Epoch 307/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.5984 - accuracy: 0.6989 - val_loss: 1.9458 - val_accuracy: 0.7550\n",
      "Epoch 308/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.1488 - accuracy: 0.7368 - val_loss: 3.0223 - val_accuracy: 0.7550\n",
      "Epoch 309/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.4052 - accuracy: 0.6745 - val_loss: 2.0699 - val_accuracy: 0.7567\n",
      "Epoch 310/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.7120 - accuracy: 0.6638 - val_loss: 1.9599 - val_accuracy: 0.7600\n",
      "Epoch 311/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.3696 - accuracy: 0.7289 - val_loss: 1.9076 - val_accuracy: 0.7550\n",
      "Epoch 312/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.3441 - accuracy: 0.6960 - val_loss: 4.8714 - val_accuracy: 0.5567\n",
      "Epoch 313/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.1222 - accuracy: 0.6831 - val_loss: 2.2774 - val_accuracy: 0.6767\n",
      "Epoch 314/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.2027 - accuracy: 0.6938 - val_loss: 2.2579 - val_accuracy: 0.7933\n",
      "Epoch 315/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.2968 - accuracy: 0.7268 - val_loss: 3.0118 - val_accuracy: 0.7400\n",
      "Epoch 316/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.3032 - accuracy: 0.7167 - val_loss: 3.7225 - val_accuracy: 0.6950\n",
      "Epoch 317/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.4058 - accuracy: 0.6602 - val_loss: 2.3502 - val_accuracy: 0.6700\n",
      "Epoch 318/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.0319 - accuracy: 0.6838 - val_loss: 1.9501 - val_accuracy: 0.7767\n",
      "Epoch 319/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.0794 - accuracy: 0.5959 - val_loss: 3.5477 - val_accuracy: 0.6017\n",
      "Epoch 320/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.9372 - accuracy: 0.6845 - val_loss: 1.7494 - val_accuracy: 0.7200\n",
      "Epoch 321/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.4686 - accuracy: 0.6366 - val_loss: 5.7001 - val_accuracy: 0.5783\n",
      "Epoch 322/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.0368 - accuracy: 0.5672 - val_loss: 4.0085 - val_accuracy: 0.5767\n",
      "Epoch 323/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.0678 - accuracy: 0.6760 - val_loss: 2.2839 - val_accuracy: 0.7883\n",
      "Epoch 324/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.9285 - accuracy: 0.6874 - val_loss: 9.2425 - val_accuracy: 0.4917\n",
      "Epoch 325/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 6.1838 - accuracy: 0.5908 - val_loss: 1.9955 - val_accuracy: 0.7800\n",
      "Epoch 326/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.3771 - accuracy: 0.7289 - val_loss: 1.8082 - val_accuracy: 0.6967\n",
      "Epoch 327/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.5561 - accuracy: 0.7046 - val_loss: 3.7508 - val_accuracy: 0.6817\n",
      "Epoch 328/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.9037 - accuracy: 0.6953 - val_loss: 2.0820 - val_accuracy: 0.7850\n",
      "Epoch 329/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.2228 - accuracy: 0.7175 - val_loss: 1.9905 - val_accuracy: 0.6800\n",
      "Epoch 330/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.4413 - accuracy: 0.6388 - val_loss: 5.3738 - val_accuracy: 0.5450\n",
      "Epoch 331/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.0871 - accuracy: 0.7275 - val_loss: 2.3091 - val_accuracy: 0.7700\n",
      "Epoch 332/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7878 - accuracy: 0.7217 - val_loss: 1.8361 - val_accuracy: 0.7783\n",
      "Epoch 333/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.5957 - accuracy: 0.7031 - val_loss: 1.5857 - val_accuracy: 0.7583\n",
      "Epoch 334/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.6983 - accuracy: 0.6001 - val_loss: 1.9757 - val_accuracy: 0.6733\n",
      "Epoch 335/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.2047 - accuracy: 0.6423 - val_loss: 5.2769 - val_accuracy: 0.5483\n",
      "Epoch 336/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.8577 - accuracy: 0.6223 - val_loss: 2.0986 - val_accuracy: 0.6567\n",
      "Epoch 337/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.1045 - accuracy: 0.7060 - val_loss: 1.6879 - val_accuracy: 0.7700\n",
      "Epoch 338/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.4167 - accuracy: 0.7010 - val_loss: 1.6878 - val_accuracy: 0.7000\n",
      "Epoch 339/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.0326 - accuracy: 0.7082 - val_loss: 3.0701 - val_accuracy: 0.7000\n",
      "Epoch 340/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.1613 - accuracy: 0.7253 - val_loss: 3.9395 - val_accuracy: 0.6567\n",
      "Epoch 341/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.6273 - accuracy: 0.6881 - val_loss: 3.7809 - val_accuracy: 0.5567\n",
      "Epoch 342/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 2.0677 - accuracy: 0.6989 - val_loss: 1.4209 - val_accuracy: 0.7517\n",
      "Epoch 343/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.8671 - accuracy: 0.7253 - val_loss: 2.5749 - val_accuracy: 0.6283\n",
      "Epoch 344/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.2759 - accuracy: 0.7003 - val_loss: 1.9784 - val_accuracy: 0.6650\n",
      "Epoch 345/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7519 - accuracy: 0.7132 - val_loss: 2.3930 - val_accuracy: 0.7350\n",
      "Epoch 346/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.1043 - accuracy: 0.6516 - val_loss: 1.5543 - val_accuracy: 0.7717\n",
      "Epoch 347/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.1422 - accuracy: 0.7060 - val_loss: 1.5499 - val_accuracy: 0.6933\n",
      "Epoch 348/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.6017 - accuracy: 0.7246 - val_loss: 2.2153 - val_accuracy: 0.6333\n",
      "Epoch 349/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.2810 - accuracy: 0.6917 - val_loss: 1.9785 - val_accuracy: 0.6517\n",
      "Epoch 350/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.4726 - accuracy: 0.6767 - val_loss: 2.2147 - val_accuracy: 0.6350\n",
      "Epoch 351/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4766 - accuracy: 0.7439 - val_loss: 3.4613 - val_accuracy: 0.5533\n",
      "Epoch 352/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.0647 - accuracy: 0.6767 - val_loss: 1.7513 - val_accuracy: 0.7783\n",
      "Epoch 353/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.2866 - accuracy: 0.6853 - val_loss: 1.2357 - val_accuracy: 0.7400\n",
      "Epoch 354/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.0054 - accuracy: 0.7039 - val_loss: 2.0317 - val_accuracy: 0.6450\n",
      "Epoch 355/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5012 - accuracy: 0.7225 - val_loss: 1.7212 - val_accuracy: 0.7767\n",
      "Epoch 356/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.6812 - accuracy: 0.7196 - val_loss: 2.1508 - val_accuracy: 0.7233\n",
      "Epoch 357/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.5364 - accuracy: 0.6631 - val_loss: 3.2571 - val_accuracy: 0.5583\n",
      "Epoch 358/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.2045 - accuracy: 0.6624 - val_loss: 1.9477 - val_accuracy: 0.7500\n",
      "Epoch 359/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.8733 - accuracy: 0.6924 - val_loss: 4.7557 - val_accuracy: 0.5533\n",
      "Epoch 360/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7601 - accuracy: 0.6953 - val_loss: 1.7407 - val_accuracy: 0.7667\n",
      "Epoch 361/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4537 - accuracy: 0.7296 - val_loss: 1.1597 - val_accuracy: 0.7183\n",
      "Epoch 362/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5016 - accuracy: 0.7260 - val_loss: 1.3706 - val_accuracy: 0.7867\n",
      "Epoch 363/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.9443 - accuracy: 0.7110 - val_loss: 9.7421 - val_accuracy: 0.4917\n",
      "Epoch 364/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.7223 - accuracy: 0.5980 - val_loss: 2.0151 - val_accuracy: 0.7200\n",
      "Epoch 365/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5336 - accuracy: 0.7182 - val_loss: 1.7370 - val_accuracy: 0.7583\n",
      "Epoch 366/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.1811 - accuracy: 0.7361 - val_loss: 1.5739 - val_accuracy: 0.7767\n",
      "Epoch 367/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5561 - accuracy: 0.7053 - val_loss: 1.1211 - val_accuracy: 0.7550\n",
      "Epoch 368/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2351 - accuracy: 0.7346 - val_loss: 2.4968 - val_accuracy: 0.6817\n",
      "Epoch 369/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.3343 - accuracy: 0.6302 - val_loss: 1.8585 - val_accuracy: 0.6350\n",
      "Epoch 370/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.5856 - accuracy: 0.6409 - val_loss: 1.6764 - val_accuracy: 0.6483\n",
      "Epoch 371/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.6189 - accuracy: 0.7146 - val_loss: 1.0427 - val_accuracy: 0.7483\n",
      "Epoch 372/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2384 - accuracy: 0.7382 - val_loss: 2.4827 - val_accuracy: 0.5617\n",
      "Epoch 373/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7302 - accuracy: 0.6996 - val_loss: 2.1864 - val_accuracy: 0.6883\n",
      "Epoch 374/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7403 - accuracy: 0.6974 - val_loss: 1.0111 - val_accuracy: 0.7400\n",
      "Epoch 375/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.1652 - accuracy: 0.7282 - val_loss: 2.1789 - val_accuracy: 0.6867\n",
      "Epoch 376/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5461 - accuracy: 0.7010 - val_loss: 1.9339 - val_accuracy: 0.7050\n",
      "Epoch 377/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3585 - accuracy: 0.7175 - val_loss: 1.1706 - val_accuracy: 0.7883\n",
      "Epoch 378/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4340 - accuracy: 0.7175 - val_loss: 1.4014 - val_accuracy: 0.6533\n",
      "Epoch 379/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3241 - accuracy: 0.7332 - val_loss: 2.1210 - val_accuracy: 0.6883\n",
      "Epoch 380/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.6116 - accuracy: 0.6931 - val_loss: 1.0107 - val_accuracy: 0.6967\n",
      "Epoch 381/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.1947 - accuracy: 0.7411 - val_loss: 1.0799 - val_accuracy: 0.7800\n",
      "Epoch 382/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2750 - accuracy: 0.7160 - val_loss: 3.1295 - val_accuracy: 0.5383\n",
      "Epoch 383/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4239 - accuracy: 0.6989 - val_loss: 0.9202 - val_accuracy: 0.7550\n",
      "Epoch 384/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.8926 - accuracy: 0.6066 - val_loss: 7.4588 - val_accuracy: 0.5083\n",
      "Epoch 385/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 3.4756 - accuracy: 0.5980 - val_loss: 3.3842 - val_accuracy: 0.5517\n",
      "Epoch 386/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.4364 - accuracy: 0.6273 - val_loss: 5.1139 - val_accuracy: 0.4917\n",
      "Epoch 387/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.3562 - accuracy: 0.6316 - val_loss: 2.1456 - val_accuracy: 0.5750\n",
      "Epoch 388/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3181 - accuracy: 0.7132 - val_loss: 0.9654 - val_accuracy: 0.7517\n",
      "Epoch 389/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0570 - accuracy: 0.7275 - val_loss: 1.0882 - val_accuracy: 0.6900\n",
      "Epoch 390/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4197 - accuracy: 0.6938 - val_loss: 1.4064 - val_accuracy: 0.7600\n",
      "Epoch 391/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3766 - accuracy: 0.7225 - val_loss: 4.2675 - val_accuracy: 0.5083\n",
      "Epoch 392/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 2.1735 - accuracy: 0.6538 - val_loss: 1.4158 - val_accuracy: 0.7517\n",
      "Epoch 393/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3434 - accuracy: 0.7031 - val_loss: 1.1703 - val_accuracy: 0.7833\n",
      "Epoch 394/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.1894 - accuracy: 0.7110 - val_loss: 0.9602 - val_accuracy: 0.6933\n",
      "Epoch 395/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8648 - accuracy: 0.7396 - val_loss: 1.0913 - val_accuracy: 0.6700\n",
      "Epoch 396/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0484 - accuracy: 0.7303 - val_loss: 1.6316 - val_accuracy: 0.6017\n",
      "Epoch 397/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0269 - accuracy: 0.7268 - val_loss: 0.9383 - val_accuracy: 0.7783\n",
      "Epoch 398/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9823 - accuracy: 0.7282 - val_loss: 1.2042 - val_accuracy: 0.7650\n",
      "Epoch 399/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0947 - accuracy: 0.7110 - val_loss: 0.8011 - val_accuracy: 0.7633\n",
      "Epoch 400/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9608 - accuracy: 0.7418 - val_loss: 1.1775 - val_accuracy: 0.7633\n",
      "Epoch 401/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9700 - accuracy: 0.7210 - val_loss: 0.8132 - val_accuracy: 0.7050\n",
      "Epoch 402/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9122 - accuracy: 0.7303 - val_loss: 0.7475 - val_accuracy: 0.7083\n",
      "Epoch 403/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9766 - accuracy: 0.7332 - val_loss: 1.2568 - val_accuracy: 0.6433\n",
      "Epoch 404/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 4.6358 - accuracy: 0.6202 - val_loss: 39.6068 - val_accuracy: 0.4917\n",
      "Epoch 405/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 23.5783 - accuracy: 0.5165 - val_loss: 51.8151 - val_accuracy: 0.4917\n",
      "Epoch 406/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 29.5560 - accuracy: 0.5093 - val_loss: 7.7523 - val_accuracy: 0.4950\n",
      "Epoch 407/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 14.2483 - accuracy: 0.5057 - val_loss: 17.4166 - val_accuracy: 0.4917\n",
      "Epoch 408/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 11.3329 - accuracy: 0.5150 - val_loss: 3.0121 - val_accuracy: 0.5083\n",
      "Epoch 409/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.8608 - accuracy: 0.5751 - val_loss: 1.6440 - val_accuracy: 0.5217\n",
      "Epoch 410/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0367 - accuracy: 0.6116 - val_loss: 0.6207 - val_accuracy: 0.7200\n",
      "Epoch 411/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7207 - accuracy: 0.6617 - val_loss: 0.5634 - val_accuracy: 0.7000\n",
      "Epoch 412/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5693 - accuracy: 0.7203 - val_loss: 0.7733 - val_accuracy: 0.5917\n",
      "Epoch 413/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5517 - accuracy: 0.7210 - val_loss: 0.5374 - val_accuracy: 0.7650\n",
      "Epoch 414/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6225 - accuracy: 0.6896 - val_loss: 0.6957 - val_accuracy: 0.6933\n",
      "Epoch 415/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7699 - accuracy: 0.6288 - val_loss: 0.5851 - val_accuracy: 0.6783\n",
      "Epoch 416/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5907 - accuracy: 0.7089 - val_loss: 0.7687 - val_accuracy: 0.5650\n",
      "Epoch 417/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5831 - accuracy: 0.6989 - val_loss: 0.5553 - val_accuracy: 0.6967\n",
      "Epoch 418/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5141 - accuracy: 0.7461 - val_loss: 0.5414 - val_accuracy: 0.7717\n",
      "Epoch 419/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5306 - accuracy: 0.7346 - val_loss: 0.6587 - val_accuracy: 0.7217\n",
      "Epoch 420/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8534 - accuracy: 0.6173 - val_loss: 0.8062 - val_accuracy: 0.6250\n",
      "Epoch 421/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7142 - accuracy: 0.6559 - val_loss: 0.5188 - val_accuracy: 0.7600\n",
      "Epoch 422/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6223 - accuracy: 0.7003 - val_loss: 0.5201 - val_accuracy: 0.7617\n",
      "Epoch 423/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6671 - accuracy: 0.6760 - val_loss: 0.5688 - val_accuracy: 0.6850\n",
      "Epoch 424/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5416 - accuracy: 0.7225 - val_loss: 0.5938 - val_accuracy: 0.6650\n",
      "Epoch 425/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6164 - accuracy: 0.6881 - val_loss: 0.5080 - val_accuracy: 0.7583\n",
      "Epoch 426/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5290 - accuracy: 0.7504 - val_loss: 0.9035 - val_accuracy: 0.5400\n",
      "Epoch 427/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6684 - accuracy: 0.6717 - val_loss: 0.5284 - val_accuracy: 0.7517\n",
      "Epoch 428/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6050 - accuracy: 0.7024 - val_loss: 0.5461 - val_accuracy: 0.7417\n",
      "Epoch 429/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5221 - accuracy: 0.7411 - val_loss: 0.5201 - val_accuracy: 0.7200\n",
      "Epoch 430/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6295 - accuracy: 0.6974 - val_loss: 0.5378 - val_accuracy: 0.7767\n",
      "Epoch 431/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5441 - accuracy: 0.7339 - val_loss: 0.5269 - val_accuracy: 0.7150\n",
      "Epoch 432/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5987 - accuracy: 0.7067 - val_loss: 0.5302 - val_accuracy: 0.7167\n",
      "Epoch 433/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5492 - accuracy: 0.7296 - val_loss: 0.5531 - val_accuracy: 0.7833\n",
      "Epoch 434/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5467 - accuracy: 0.7318 - val_loss: 0.6569 - val_accuracy: 0.6967\n",
      "Epoch 435/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6166 - accuracy: 0.6931 - val_loss: 0.5108 - val_accuracy: 0.7733\n",
      "Epoch 436/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5269 - accuracy: 0.7332 - val_loss: 0.4994 - val_accuracy: 0.7500\n",
      "Epoch 437/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5718 - accuracy: 0.7089 - val_loss: 0.7229 - val_accuracy: 0.6150\n",
      "Epoch 438/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5488 - accuracy: 0.7289 - val_loss: 0.5918 - val_accuracy: 0.7433\n",
      "Epoch 439/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5184 - accuracy: 0.7454 - val_loss: 0.5007 - val_accuracy: 0.7533\n",
      "Epoch 440/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5259 - accuracy: 0.7396 - val_loss: 0.8483 - val_accuracy: 0.5517\n",
      "Epoch 441/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6005 - accuracy: 0.6881 - val_loss: 0.5374 - val_accuracy: 0.6967\n",
      "Epoch 442/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5687 - accuracy: 0.7110 - val_loss: 0.5221 - val_accuracy: 0.7617\n",
      "Epoch 443/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6890 - accuracy: 0.6595 - val_loss: 1.1644 - val_accuracy: 0.5583\n",
      "Epoch 444/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5595 - accuracy: 0.7332 - val_loss: 0.5344 - val_accuracy: 0.7267\n",
      "Epoch 445/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6049 - accuracy: 0.7003 - val_loss: 0.5607 - val_accuracy: 0.6900\n",
      "Epoch 446/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5485 - accuracy: 0.7189 - val_loss: 0.7414 - val_accuracy: 0.5667\n",
      "Epoch 447/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6071 - accuracy: 0.6731 - val_loss: 0.7098 - val_accuracy: 0.6567\n",
      "Epoch 448/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2908 - accuracy: 0.5715 - val_loss: 0.8462 - val_accuracy: 0.5800\n",
      "Epoch 449/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5561 - accuracy: 0.5873 - val_loss: 0.7182 - val_accuracy: 0.6200\n",
      "Epoch 450/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5642 - accuracy: 0.5472 - val_loss: 2.0389 - val_accuracy: 0.4917\n",
      "Epoch 451/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8614 - accuracy: 0.5236 - val_loss: 0.6237 - val_accuracy: 0.6500\n",
      "Epoch 452/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8694 - accuracy: 0.5072 - val_loss: 0.7797 - val_accuracy: 0.4933\n",
      "Epoch 453/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9423 - accuracy: 0.5186 - val_loss: 0.8708 - val_accuracy: 0.5083\n",
      "Epoch 454/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7328 - accuracy: 0.6223 - val_loss: 0.7323 - val_accuracy: 0.5083\n",
      "Epoch 455/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7452 - accuracy: 0.6066 - val_loss: 0.6887 - val_accuracy: 0.5933\n",
      "Epoch 456/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6896 - accuracy: 0.6252 - val_loss: 0.5621 - val_accuracy: 0.7183\n",
      "Epoch 457/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.6974 - val_loss: 0.5440 - val_accuracy: 0.7383\n",
      "Epoch 458/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6656 - accuracy: 0.6252 - val_loss: 0.6030 - val_accuracy: 0.6100\n",
      "Epoch 459/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6803 - accuracy: 0.6152 - val_loss: 0.5627 - val_accuracy: 0.7550\n",
      "Epoch 460/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7313 - accuracy: 0.5937 - val_loss: 0.5398 - val_accuracy: 0.7633\n",
      "Epoch 461/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6034 - accuracy: 0.6795 - val_loss: 0.6023 - val_accuracy: 0.6017\n",
      "Epoch 462/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6058 - accuracy: 0.6502 - val_loss: 0.5421 - val_accuracy: 0.7600\n",
      "Epoch 463/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6102 - accuracy: 0.6674 - val_loss: 0.5593 - val_accuracy: 0.7633\n",
      "Epoch 464/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5813 - accuracy: 0.6981 - val_loss: 0.6376 - val_accuracy: 0.5700\n",
      "Epoch 465/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5640 - accuracy: 0.7182 - val_loss: 0.5301 - val_accuracy: 0.7533\n",
      "Epoch 466/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5663 - accuracy: 0.7146 - val_loss: 0.5400 - val_accuracy: 0.7050\n",
      "Epoch 467/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5999 - accuracy: 0.6888 - val_loss: 0.5537 - val_accuracy: 0.6800\n",
      "Epoch 468/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6894 - accuracy: 0.6102 - val_loss: 0.5279 - val_accuracy: 0.7367\n",
      "Epoch 469/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5616 - accuracy: 0.7303 - val_loss: 0.5714 - val_accuracy: 0.6633\n",
      "Epoch 470/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6425 - accuracy: 0.6273 - val_loss: 0.6784 - val_accuracy: 0.6517\n",
      "Epoch 471/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6940 - accuracy: 0.6001 - val_loss: 0.6638 - val_accuracy: 0.6550\n",
      "Epoch 472/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5653 - accuracy: 0.7124 - val_loss: 0.5425 - val_accuracy: 0.7150\n",
      "Epoch 473/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5874 - accuracy: 0.7039 - val_loss: 0.6093 - val_accuracy: 0.6133\n",
      "Epoch 474/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6558 - accuracy: 0.6338 - val_loss: 0.5328 - val_accuracy: 0.7533\n",
      "Epoch 475/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.6803 - val_loss: 0.6186 - val_accuracy: 0.6900\n",
      "Epoch 476/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6586 - accuracy: 0.6338 - val_loss: 0.5507 - val_accuracy: 0.7167\n",
      "Epoch 477/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5530 - accuracy: 0.7239 - val_loss: 0.5403 - val_accuracy: 0.7550\n",
      "Epoch 478/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5537 - accuracy: 0.7260 - val_loss: 0.5622 - val_accuracy: 0.6767\n",
      "Epoch 479/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7268 - val_loss: 0.5792 - val_accuracy: 0.6600\n",
      "Epoch 480/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5363 - accuracy: 0.7411 - val_loss: 0.5908 - val_accuracy: 0.6467\n",
      "Epoch 481/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5496 - accuracy: 0.7289 - val_loss: 0.6139 - val_accuracy: 0.6233\n",
      "Epoch 482/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6022 - accuracy: 0.6903 - val_loss: 0.5268 - val_accuracy: 0.7633\n",
      "Epoch 483/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5218 - accuracy: 0.7439 - val_loss: 0.5978 - val_accuracy: 0.7117\n",
      "Epoch 484/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5371 - accuracy: 0.7289 - val_loss: 0.7078 - val_accuracy: 0.5533\n",
      "Epoch 485/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6832 - accuracy: 0.6130 - val_loss: 0.6178 - val_accuracy: 0.6150\n",
      "Epoch 486/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6124 - accuracy: 0.6595 - val_loss: 0.5427 - val_accuracy: 0.7167\n",
      "Epoch 487/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5616 - accuracy: 0.7196 - val_loss: 0.5544 - val_accuracy: 0.6883\n",
      "Epoch 488/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5456 - accuracy: 0.7346 - val_loss: 0.5477 - val_accuracy: 0.7000\n",
      "Epoch 489/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5395 - accuracy: 0.7353 - val_loss: 0.6792 - val_accuracy: 0.6617\n",
      "Epoch 490/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5510 - accuracy: 0.7175 - val_loss: 0.5377 - val_accuracy: 0.7900\n",
      "Epoch 491/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5599 - accuracy: 0.7146 - val_loss: 0.5148 - val_accuracy: 0.7267\n",
      "Epoch 492/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5441 - accuracy: 0.7189 - val_loss: 0.6665 - val_accuracy: 0.6617\n",
      "Epoch 493/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6761 - accuracy: 0.5923 - val_loss: 0.6972 - val_accuracy: 0.4917\n",
      "Epoch 494/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5451 - val_loss: 0.6932 - val_accuracy: 0.4917\n",
      "Epoch 495/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6891 - accuracy: 0.5036 - val_loss: 0.6888 - val_accuracy: 0.4917\n",
      "Epoch 496/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6846 - accuracy: 0.5036 - val_loss: 0.6831 - val_accuracy: 0.4917\n",
      "Epoch 497/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6738 - accuracy: 0.5973 - val_loss: 0.6692 - val_accuracy: 0.7533\n",
      "Epoch 498/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6595 - accuracy: 0.6888 - val_loss: 0.6549 - val_accuracy: 0.6533\n",
      "Epoch 499/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6461 - accuracy: 0.7139 - val_loss: 0.6460 - val_accuracy: 0.6617\n",
      "Epoch 500/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6423 - accuracy: 0.6910 - val_loss: 0.6404 - val_accuracy: 0.6550\n",
      "Epoch 501/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6349 - accuracy: 0.6960 - val_loss: 0.6502 - val_accuracy: 0.5750\n",
      "Epoch 502/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6273 - accuracy: 0.7217 - val_loss: 0.6271 - val_accuracy: 0.7100\n",
      "Epoch 503/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6253 - accuracy: 0.7060 - val_loss: 0.6551 - val_accuracy: 0.5567\n",
      "Epoch 504/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6365 - accuracy: 0.6710 - val_loss: 0.6692 - val_accuracy: 0.6283\n",
      "Epoch 505/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6263 - accuracy: 0.6924 - val_loss: 0.6222 - val_accuracy: 0.6817\n",
      "Epoch 506/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6103 - accuracy: 0.7353 - val_loss: 0.6439 - val_accuracy: 0.6083\n",
      "Epoch 507/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6168 - accuracy: 0.6960 - val_loss: 0.6084 - val_accuracy: 0.7550\n",
      "Epoch 508/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5971 - accuracy: 0.7461 - val_loss: 0.6011 - val_accuracy: 0.7467\n",
      "Epoch 509/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6034 - accuracy: 0.7253 - val_loss: 0.6033 - val_accuracy: 0.7417\n",
      "Epoch 510/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6099 - accuracy: 0.7203 - val_loss: 0.6323 - val_accuracy: 0.6517\n",
      "Epoch 511/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5983 - accuracy: 0.7239 - val_loss: 0.6214 - val_accuracy: 0.6767\n",
      "Epoch 512/512\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5938 - accuracy: 0.7403 - val_loss: 0.6715 - val_accuracy: 0.5767\n"
     ]
    }
   ],
   "source": [
    "discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.legacy.Adam(), metrics=['accuracy'])\n",
    "\n",
    "v = discriminator.fit([np.array(state_train),np.array(action_train)],np.array(label_train),validation_data = ([np.array(state_test),np.array(action_test)],np.array(label_test)) , epochs=512, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6684e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cabedd50>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABQtklEQVR4nO3deXhU5f3+8feZNQtZWBPCoqi4oKACilG7qCgqbbXaxf5oy7e12lpo3arVVq1WW9TaaqnWrVZs1WK14kKViqC4scmibLLIFpYkQMiezHae3x+TzJxJJiGBkAzkfl1XLmfOOXPmmQNmbj7PcixjjEFEREQkhbi6ugEiIiIiTSmgiIiISMpRQBEREZGUo4AiIiIiKUcBRURERFKOAoqIiIikHAUUERERSTkKKCIiIpJyPF3dgP1h2zY7duwgKysLy7K6ujkiIiLSBsYYqqqqKCgowOVqvUZySAaUHTt2MGjQoK5uhoiIiOyHoqIiBg4c2Ooxh2RAycrKAqIfMDs7u4tbIyIiIm1RWVnJoEGDYt/jrTkkA0pjt052drYCioiIyCGmLcMzNEhWREREUo4CioiIiKQcBRQRERFJOQooIiIiknIUUERERCTlKKCIiIhIylFAERERkZSjgCIiIiIpRwFFREREUo4CioiIiKQcBRQRERFJOQooIiIiknIUUJooqwny2LzPKams7+qmiIiIdFsKKE1cO30Z9775Gd9/alFXN0VERKTbUkBp4v31uwFYW1LVxS0RERHpvhRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSctoVUCKRCLfffjtDhgwhPT2do48+mrvvvhtjTOwYYwx33HEH/fv3Jz09nbFjx7J+/fqE85SVlTFhwgSys7PJzc3lyiuvpLq6umM+kYiIiBzy2hVQ7rvvPh599FEefvhh1qxZw3333cf999/PX/7yl9gx999/P1OnTuWxxx5j4cKFZGZmMm7cOOrr62PHTJgwgVWrVjF79mxmzpzJe++9x9VXX91xn0pEREQOaZZxlj/24Stf+Qp5eXk89dRTsW2XX3456enpPPvssxhjKCgo4MYbb+QXv/gFABUVFeTl5TFt2jSuuOIK1qxZw7Bhw1i8eDGjR48GYNasWVx88cVs27aNgoKCfbajsrKSnJwcKioqyM7Obu9nbtWRt/w39njzveM79NwiIiLdWXu+v9tVQTnzzDOZM2cO69atA+CTTz7hgw8+4KKLLgJg06ZNFBcXM3bs2NhrcnJyGDNmDPPnzwdg/vz55ObmxsIJwNixY3G5XCxcuDDp+wYCASorKxN+RERE5PDlac/Bt9xyC5WVlRx//PG43W4ikQi/+93vmDBhAgDFxcUA5OXlJbwuLy8vtq+4uJh+/folNsLjoVevXrFjmpoyZQp33XVXe5oqIiIih7B2VVD+/e9/89xzz/H888+zdOlSnnnmGR544AGeeeaZg9U+AG699VYqKipiP0VFRQf1/URERKRrtauCctNNN3HLLbdwxRVXADB8+HC2bNnClClTmDhxIvn5+QCUlJTQv3//2OtKSko45ZRTAMjPz6e0tDThvOFwmLKystjrm/L7/fj9/vY0VURERA5h7aqg1NbW4nIlvsTtdmPbNgBDhgwhPz+fOXPmxPZXVlaycOFCCgsLASgsLKS8vJwlS5bEjpk7dy62bTNmzJj9/iAiIiJy+GhXBeWrX/0qv/vd7xg8eDAnnngiy5Yt409/+hM//OEPAbAsi+uuu4577rmHoUOHMmTIEG6//XYKCgq49NJLATjhhBO48MILueqqq3jssccIhUJMnjyZK664ok0zeEREROTw166A8pe//IXbb7+dn/70p5SWllJQUMCPf/xj7rjjjtgxN998MzU1NVx99dWUl5dz9tlnM2vWLNLS0mLHPPfcc0yePJnzzjsPl8vF5ZdfztSpUzvuU4mIiMghrV3roKQKrYMiIiJy6Dlo66CIiIiIdAYFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSctodULZv3853v/tdevfuTXp6OsOHD+fjjz+O7TfGcMcdd9C/f3/S09MZO3Ys69evTzhHWVkZEyZMIDs7m9zcXK688kqqq6sP/NOIiIjIYaFdAWXv3r2cddZZeL1e3nzzTVavXs0f//hHevbsGTvm/vvvZ+rUqTz22GMsXLiQzMxMxo0bR319feyYCRMmsGrVKmbPns3MmTN57733uPrqqzvuU4mIiMghzTLGmLYefMstt/Dhhx/y/vvvJ91vjKGgoIAbb7yRX/ziFwBUVFSQl5fHtGnTuOKKK1izZg3Dhg1j8eLFjB49GoBZs2Zx8cUXs23bNgoKCvbZjsrKSnJycqioqCA7O7utzW+TI2/5b+zx5nvHd+i5RUREurP2fH+3q4Ly2muvMXr0aL75zW/Sr18/Tj31VJ588snY/k2bNlFcXMzYsWNj23JychgzZgzz588HYP78+eTm5sbCCcDYsWNxuVwsXLgw6fsGAgEqKysTfkREROTw1a6AsnHjRh599FGGDh3K//73P6655hp+/vOf88wzzwBQXFwMQF5eXsLr8vLyYvuKi4vp169fwn6Px0OvXr1ixzQ1ZcoUcnJyYj+DBg1qT7NFRETkENOugGLbNiNHjuT3v/89p556KldffTVXXXUVjz322MFqHwC33norFRUVsZ+ioqKD+n4iIiLStdoVUPr378+wYcMStp1wwgls3boVgPz8fABKSkoSjikpKYnty8/Pp7S0NGF/OBymrKwsdkxTfr+f7OzshB8RERE5fLUroJx11lmsXbs2Ydu6des44ogjABgyZAj5+fnMmTMntr+yspKFCxdSWFgIQGFhIeXl5SxZsiR2zNy5c7FtmzFjxuz3BxEREZHDh6c9B19//fWceeaZ/P73v+db3/oWixYt4oknnuCJJ54AwLIsrrvuOu655x6GDh3KkCFDuP322ykoKODSSy8FohWXCy+8MNY1FAqFmDx5MldccUWbZvCIiIjI4a9dAeW0005jxowZ3Hrrrfz2t79lyJAhPPTQQ0yYMCF2zM0330xNTQ1XX3015eXlnH322cyaNYu0tLTYMc899xyTJ0/mvPPOw+VycfnllzN16tSO+1QiIiJySGvXOiipQuugiIiIHHoO2jooIiIiIp1BAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSjgKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooLTCGNPVTRAREemWFFBaoXwiIiLSNRRQREREJOUooLRCBRQREZGuoYDSCo1BERER6RoKKK1QPBEREekaCigiIiKSchRQWqEeHhERka6hgNIKo04eERGRLqGAIiIiIilHAaUV6uIRERHpGgooIiIiknIUUFqhCoqIiEjXUEARERGRlKOA0grN4hEREekaCiitUBePiIhI11BAERERkZSjgNIKFVBERES6hgJKK3Q3YxERka6hgNIKxRMREZGuoYAiIiIiKUcBpRXq4REREekaCiitUUARERHpEgooIiIiknIUUFqhlWRFRES6hgJKKzQGRUREpGsooLRC+URERKRrKKCIiIhIylFAaYVWkhUREekaCiitUDwRERHpGgooDk0rJiqgiIiIdA0FFAcFEhERkdSggNIKrYMiIiLSNRRQHJrFEeUTERGRLqGA4qBZOyIiIqlBAaUViisiIiJdQwHFoWkgUUFFRESkayigODQNJBokKyIi0jUOKKDce++9WJbFddddF9tWX1/PpEmT6N27Nz169ODyyy+npKQk4XVbt25l/PjxZGRk0K9fP2666SbC4fCBNEVEREQOI/sdUBYvXszjjz/OiBEjErZff/31vP7667z44ovMmzePHTt2cNlll8X2RyIRxo8fTzAY5KOPPuKZZ55h2rRp3HHHHfv/KTpI04qJMfDkexu59eUVGkArIiLSifYroFRXVzNhwgSefPJJevbsGdteUVHBU089xZ/+9CfOPfdcRo0axdNPP81HH33EggULAHjrrbdYvXo1zz77LKeccgoXXXQRd999N4888gjBYLBjPtV+MgYK2M1dnqcZYu3EAL97Yw3/WrSVxZv3dmnbREREupP9CiiTJk1i/PjxjB07NmH7kiVLCIVCCduPP/54Bg8ezPz58wGYP38+w4cPJy8vL3bMuHHjqKysZNWqVUnfLxAIUFlZmfBzsDzp+yMTPbP5l++ehO01AXVBiYiIdBZPe18wffp0li5dyuLFi5vtKy4uxufzkZubm7A9Ly+P4uLi2DHOcNK4v3FfMlOmTOGuu+5qb1P3y4muLQDkW3vZ5ujWsdXFIyIi0mnaVUEpKiri2muv5bnnniMtLe1gtamZW2+9lYqKithPUVHRQXmfZrN4HM9t5RMREZFO066AsmTJEkpLSxk5ciQejwePx8O8efOYOnUqHo+HvLw8gsEg5eXlCa8rKSkhPz8fgPz8/GazehqfNx7TlN/vJzs7O+GnM2wvr4s91iBZERGRztOugHLeeeexYsUKli9fHvsZPXo0EyZMiD32er3MmTMn9pq1a9eydetWCgsLASgsLGTFihWUlpbGjpk9ezbZ2dkMGzasgz7W/mk6i+eKJxbEHpfVBDn3gXf589vrO7tZIiIi3U67xqBkZWVx0kknJWzLzMykd+/ese1XXnklN9xwA7169SI7O5uf/exnFBYWcsYZZwBwwQUXMGzYML73ve9x//33U1xczG233cakSZPw+/0d9LH2T2tFkife28jG3TU8+PY6rh07tPMaJSIi0g21e5Dsvjz44IO4XC4uv/xyAoEA48aN469//Wtsv9vtZubMmVxzzTUUFhaSmZnJxIkT+e1vf9vRTWm31jpxghG709ohIiLS3VnmEBxcUVlZSU5ODhUVFR06HqU6EKbHlN6x50fWPx97PCA3PTYmZfO94zvsPUVERLqL9nx/6148Dq1ltUMwx4mIiByyFFAcnBEkZNwt7hMREZGDSwGlBWGaBBQlFBERkU6jgOLgDCGhJgFFREREOo8CipMjoDSroKiTR0REpNMooLQg0iSgaKl7ERGRzqOA4uCskjTt4tEYFBERkc6jgOJg7PhibGHTdAyKEoqIiEhnUUBxCgdiD0NNFtmNqI9HRESk0yigOIXr4w+bdPFEbEM69VzveRGKV3Z2y0RERLoVBRQH4wgoTUVsw82eF7jWMwMeO6sTWyUiItL9KKA4WI6A4iHCZPcMXvf9ih7UEjGGs1yqnIiIiHQGBRQnR0BxYfML74sMd23me+63sW3IsWq6sHEiIiLdhwKKk2OQrIf4jB6/FSRiDNnUdkWrREREuh0FFCdnBcWKBxSLhkGyVrALGgW3vbKCHzy9CFsziUREpJvw7PuQbiRcF3voIdKFDUn07IKtAHy6vYJTBuV2bWNEREQ6gSooDpaji8ft6OJJlUXatBaLiIh0FwooTi0GlETGm9EZrREREem2FFAcnNOM3U26eDyEY49tf06ntcnoJkAiItINKaA4JayD4hwka8glPsXY+LI6rUnq1RERke5IAcXB9vWgyqQD4Cdxxk5Pq8rxrPNSg60KioiIdEMKKA71x13KeYEHAHBb8WBgAT1xBBQ7TGdRQBERke5IAcXBGIi0cEkS1kCxO28Kst3yWF0REZHDlgKKgyF5QLEwuJyzerqogmJZnfa2IiIiXUoLtTURwd1s22TPqyyyj4tvMJ1X1nAGFPX2iIhId6EKioMxpsUuntNda+NPOrWC0mlvJSIikjIUUBxaG4OSeGDnjUHROigiItIdKaA0EU7SxdOU1ZmDZJVPRESkG1JAaSLVKiiJ04yVVkREpHtQQHGIZgGLiGl9uozVmWNQbA2SFRGR7kcBxcE0VCj2WUXp1Fk8yR+LiIgczhRQkkg21dipUysojrKJVpUVEZHuQgHFofH7f18DZS1Mpy3xmhBQVEIREZFuQgHFofHr36YNS7Z20kBZoy4eERHphhRQkmjLVOPOuh+PunhERKQ7UkBxaFwUzW7LZemkcSgRWwFFRES6HwUUh8av/zZVUDqpi8fZraN8IiIi3YUCShJtWqytk7p4jLp4RESkG1JAcWj8/o+Y1AkoWgdFRES6IwWUBNEE0LZBsp0zBkWDZEVEpDtSQEmiTYNkO20MitZBERGR7kcBxSG+UFvqzOJxrgenfCIiIt2FAopDfKG2fV+WB2atZkd53cFtEOriERGR7kkBJYm2jEF589NtXPPc0oPeFgUUERHpjhRQHGKzeNpwWdzYrNpecZBbpHVQRESke1JAcTANnTxtDSidUdHQOigiItIdKaA4xCso++7icROhM+KC1kEREZHuSAElibYs1ObG7pQuF41BERGR7kgBxaG9Y1A6g3PtE62DIiIi3YUCioNpx0qynRZQ1MUjIiLdkAJKEm2poHisLlhJVl08IiLSTSigOCQbJHtr6EoW2sc3O9bVaRWUeCgxCigiItJNKKAkEcGKPQ7hSTpo1tNJAcWoi0dERLohBZQknEvdV5jMpF0+XVFBURePiIh0FwooDo3f/ydZm2LbPrBPSnpvHg+dMwYl4pzFo3wiIiLdhAKKQ+MsHtPQxRMybupI6+IKiuOxEoqIiHQTCihJ3BT6Me9GTub84P1A8lk9nTcGRV08IiLS/Xi6ugGppPH7f4k5jv8L/TK2PVkXj9ZBEREROXhUQXFo6fs/WQXF3UljUDRIVkREuiMFlDZIWkGxtA6KiIjIwdKugDJlyhROO+00srKy6NevH5deeilr165NOKa+vp5JkybRu3dvevToweWXX05JSUnCMVu3bmX8+PFkZGTQr18/brrpJsLh8IF/mgPUUgBIXkHpimnGnfKWIiIiXa5dAWXevHlMmjSJBQsWMHv2bEKhEBdccAE1NTWxY66//npef/11XnzxRebNm8eOHTu47LLLYvsjkQjjx48nGAzy0Ucf8cwzzzBt2jTuuOOOjvtU+6l9XTyddbNAx2NVUEREpJto1yDZWbNmJTyfNm0a/fr1Y8mSJXzxi1+koqKCp556iueff55zzz0XgKeffpoTTjiBBQsWcMYZZ/DWW2+xevVq3n77bfLy8jjllFO4++67+eUvf8mdd96Jz+fruE/XTi19/xvHyrKNWh2DEqyFrfPhyLPB4z+gNqmCIiIi3dEBjUGpqKgAoFevXgAsWbKEUCjE2LFjY8ccf/zxDB48mPnz5wMwf/58hg8fTl5eXuyYcePGUVlZyapVq5K+TyAQoLKyMuGnMyVb6r7FCkrdXpjxY3j2Mpjz2wN+b6N1UEREpBva74Bi2zbXXXcdZ511FieddBIAxcXF+Hw+cnNzE47Ny8ujuLg4dowznDTub9yXzJQpU8jJyYn9DBo0aH+bvQ/7HoMSMtEbCSatoBQthvuOhDWvRZ8veqLldzKG0sr6fbZIs3hERKQ72u+AMmnSJFauXMn06dM7sj1J3XrrrVRUVMR+ioqKDsr7tPT975zFE2roFXNjc5K1ERY+Hh8o8t79iS/09YDqXVD6WbNz3vKfFZz++zm8sWJnq23SOigiItId7VdAmTx5MjNnzuSdd95h4MCBse35+fkEg0HKy8sTji8pKSE/Pz92TNNZPY3PG49pyu/3k52dnfDTmZwVlGBDQPFgM9N/G7x5M6x4Mbqz6XgTfw944Bj46xgo25iw64WPoyHrwdnrWn1vTTMWEZHuqF0BxRjD5MmTmTFjBnPnzmXIkCEJ+0eNGoXX62XOnDmxbWvXrmXr1q0UFhYCUFhYyIoVKygtLY0dM3v2bLKzsxk2bNiBfJYD1pZZPI0VlIR78ZSsjP7X3SSg+LLij4sW7/M9Pykq5+PNZXzniQXMW7cLUBePiIh0T+2axTNp0iSef/55Xn31VbKysmJjRnJyckhPTycnJ4crr7ySG264gV69epGdnc3PfvYzCgsLOeOMMwC44IILGDZsGN/73ve4//77KS4u5rbbbmPSpEn4/Qc24+VAtaWLJ1ZBsRxjUKyGWT7JKigN6kJh0lt571U7KrjkkQ9jz+dv3MPme8cnDIxVF4+IiHQX7Qoojz76KABf/vKXE7Y//fTT/N///R8ADz74IC6Xi8svv5xAIMC4ceP461//GjvW7XYzc+ZMrrnmGgoLC8nMzGTixIn89rcHPuPlYEkcJOsBq4W7GVtNClK+eEBZs6OCkUnO3TiBefGmsqTvnTgGRQlFRES6h3YFlLaMgUhLS+ORRx7hkUceafGYI444gjfeeKM9b90pWvp8SSsoyQJKqDbxuTdeM2m+kkrDezbut5IfkTgGpYWTiIiIHGZ0Lx6HlsegxMND0jEojQLViecLB2KPXVbr6aKFfJIQSiLq4xERkW5CAaUNEmfxeAHwJFkHxQQqmzyPBxZXSyWUBi3t1iBZERHpjhRQHNozSDaNoOOIaLzYU5Y4jsR2BpR9vHdLXTwRLXUvIiLdkAKKg2lpJVnTfCXZDCvefWOAu2euprqyPPGFwfhNFLGDtKYtXTxaB0VERLoLBRSnlm4WaLljjwNEb2bYg7rYtuKKOp76YBOZVl3iCx0Bpf+W11i74A1KWlje3mqhkydxmrECioiIdA8KKG1gHNOHa0gDIMeKh49AIBo6skgMKFYo3sXTa89Sjpv1Hcb8Pr6I3QB28YXQR2BMixUULXUvIiLdUbumGR/u2rKSbLWJTh3OxTFjJ1yPmwhpVgiAnwcnM9X3MO5QDa15338drjoDK47G4oykx2iQrIiIdEeqoDi0+P3v6OKpblgPNteKB5QjN73As94pseflZLb4Hs67IDdOPQ6ue7uVNhksbDyEtQ6KiIh0GwoobWDvq4ICFLpXAxAwXmpMWovn8hFqtu31T3ZQ3MLYlIgxTPfdw7v+G7Ac66qIiIgczhRQHFqaxYOr+RgUvxVOemg1aYRxJ90H4CPZ6wxvrSpJsj06SHaM6zMGWrsZUL+2xfOKiIgcThRQHFru4olfpqpWb/kHNSYtttpsMj5CCTNzAFwYAuHmC78BWHa8amKZJKvXioiIHIYUUNrAdlREGrt4WlJLWmwxt2R8hAlGEoOGhaE+FN3Wj70J41TcEcf6KXbyECMiInK4UUBxaKmAYhxdPNX7qKAE8LbexWOFCYQSA0pjBWWktY5FaZN40vvH2D53OD42xW1aX+xNRETkcKGA4tDSSq0mySDZlgTwEjKtd/E07c5xYQiEbH7omQXAue7l8X2OLh6vnXwgrYiIyOFGAaUNnCvJNg6SbUnQePbZxRMI2wndOGAIhG38SWb4uCPxUOKP1La90SIiIocwBRSHFrt4rLaPQQng28csnmgFxTnd2MIQjNhJpyA7Kyg+u67ZfhERkcORVpJ1asNCbQF8hIwbr5V8wGoQT+uzeKww9aHEasl49yKW2zObTUEurw2ytmhX/LUKKCIi0k2ogtIWjkGyESzq8Ld4aABvqwHFT4hA2G4WRn7tfZ6jXDsStl305/fZXV4Re+7TGBQREekmFFAcWlqozXkfvwgualsJKEHTekDxESIQiuC3ms/I6UVVwvOdFfX4iR/nVwVFRES6CQUUh5YWanM5EorBRZ3xtXiOAF5sXAl3QHbyEaY2GEk6IDZZt1Ga4zgFFBER6S4UUBxaCiheT3wMSgQXda3M5AnijT5wJw8xXsLUBMP4ky55H3eL518UsJs0ZwXFpFBAMQbKNray/K6IiMj+U0Bpg2H9s2OP99XFE2gMKK7k3Tw+K0xdMJLQdZPMTzyv85TvAfxWilZQ3nsApp4K8+7r6paIiMhhSAHFoaVagMvRx2Pjota0MgalcfxJCxUUHyFqgpGE4NGSE1xbEyooaalUQXnnnuh/353Ste0QEZHDkgKKQ0sryTpFu3haqaA0jE8xbm/S/T5C1AbCLdzVuLmELh7N4hERkW5CAaWdbCzqaH2QLIBxJQ8ofsLUhvbdxRM73nKOQVFAERGR7kEBxaFtwz0tykx2i3sbu3haCig+K1pBSTaLJxnncSnVxdPCGBsREZGOoG8Zh5Z6eDJ9iUvXPxkeTxXpbDN9ucPzT7KseHAIW9HqSsTyJr24PsLUBCP71cWTnkoVFLcf7LZ9BhERkfZSBWUfxo/oz9lD+yRs20lvHgx/kxcjX2ZM4BGW9v5KbF+4oXLivB/PL0NX8Uz4fCA6BqWujYNkgdQdJOtpeRyOiIjIgVJASdC8hPLI/xuJ19XyZaoljeq0/rHnYSv6xe1cTbbSZFBODwDGupbiqt3VjjEo8SDjIwTVu1o5urkHZ69j7J/mUV7btvdrMwUUERE5iBRQHPZ3zTHL8WUdaaigBE28ghLCQ9BEtx/pKuHXxT/fry4eAHYsbVfb/jxnPRtKq/nH/C3tel1rjDGELJ9zQ4edW0REBBRQWmVZ+z4GSKgmNI5BCUfs2Ladpld8fRQgP1JMhtW28STNAsr29gWURnWh5Hdf3h8vLC5iU7kjYIVTaGzMIaayvm1dfSIi3Y0CikPTOkAsn/izWn2dyxOvJkRc0cd5Vati21abI+JL4Dc41drQpjY13otnkX1cdMP2JW16XVN2B1Y5Hn5nA2HnEOD6yg47d3cya2UxI+58iwdnr+vqpoiIpBwFFIem3+FWYwnlqHNgxBVwwT1JX5fYxRN97CZesTC4EiooAGe5VgIwN3IK94auiG1fZw9IOC69odKyxD42uqF0dRs/TZOF5zqwF8ZlWXicXVSBqpYPPhjKNsKDw2Hh4537vh3s1zNWANFuOBERSaSA0opYBcXlgssehzN/lvzAJGNQXs78NgBPHxG9V03jGJRGbiuaGFaYo3gqcjGf2/1ZaB/PXhKrNbnUALDN9I1uqNvb5vbXBuMhqSMrKJZF4hiaQEWHnbtNZt8BFVvhzZs79307WJu7EEVEuiEFFAfTpMzQ1i+QgIlXRxorKA/zHUbWP8aegnMAmlVQnK8N4eH84B/4dvB2XNgJ+3ta0erELpMT3RCqhXDbZuTUBOMhImx3XEBxWRZeqwsrKG38/CIicuhSQHFosYtnH+ptZ0CJjkHZUxuijGz6ZkUDS9MxKI0al8a3cQEW7iYBJceqBaDY9HK8qG1jPmoC8QpKdX3HLapm0TDluVFnj0GxDpe/tiqhiIi05HD5Td8hWhwkuw91kfhltN3RQFLVMDsj0+8h3etOcvaoQJP7+jStoDTabXKoIT36pL5tXSo1gXgoqQ504KqvVvSeQjGdXUE5TAKKunhERFp2ePymP0j29QVyRO8MAE49Il7dMA0VlMYelXSvm0y/m55WddJzVJv0hOeeFgJKGVlUkRl9Ul++j5ZHNQaUAeyid9VnbXpNWzSvoHTyGJTD5Jv98PgUIiIHh+7F42Ca9PFY+/gKeePnX6Cksp4hFQti2yJuPziqC2leF5l+D7tqc5Keo4rEgNK0iweg1vipx0+VlUG+oc1dKo1jUD5MuxZKgL1nQ88j2vTa1rgsC6+zgtKBAcW2Df+Yv5lRR/Ri+MDk1wyXO/l2ERE5bKiC0grXPv6Jm+n3cFTfHom9N03uYpzmdZPp8zDXPpX7Qlfw5uAbE/ZXm4zE90wSUMoaZvZUmcYKStsCQXUgkni+7R83OyYQjrCzon33+HETic1CiranvF2vb82sVcXc+fpqvvrwBy0f5OziieiGhSIihyMFlFa0dZCsM6G43YmXNM3rpoffg8HFo5GvsbPfOQn7q9tQQdlrovfxiXfxtBJQNn8AVSUA1AbC5ODoWgo1DyJXPLGAwilzWbOz7QNd/VaTUFBX3ubX7svGXcm7whI4A0qo5sDecNdamP0bqC07sPPsh8Okp0pE5KBQQHFoNounrS/sdVTsoced+Ko0r4sMf7xLwt8jcZ2TyiYBJVkFZa9pqKDQUG1pKaBsXwrTxsMrPwGiA2N7WY4BrFXFzV6ybGs5ADM/3ZH8nEl4TZPl2duxNsu+5KTHK1AtLgNvHNco2HJAMcbw+a7qZl13CZ75Gnz4EMy8rn0NFRGRg0oBxWF/10Gh99Ew4SW4eh5uV/MKSqY/PtQnLTM7YX/TQbJ3hSc2O31jF0/lvioouxoGwu75HIhOM+5JPKDYFdsTDo841kbJTks+DbrRrS+vYNJzSzHG4GtaQenALh5nm3aWt3CPn5Bje7C2xXP98a11nPfHefxlbiu3FahuCG1rZranmR1iX2OcRES6MwWUVrS9iwcYej4UnIKnycCVdK+bHr54QMnKSBxzUk3i854jLuIrmc/zRHh8bFtjBaWyaQVlwxx4+er486qd0f82dFfUBhMrKJHyooT32l0diD3O8Lc8XjpYso4tH7/Bf1fsZENpNV5z8Lp4ahyr3+4ob94lVR+KsHpriaNxLXcJPfxONJj8qYV73Wx3nt903M0U90erVR4RkW5IAcWh6XfE104uaPc5mgaUNK87oYsnOz2xUhFqMpHqxguO49gjBrDAPiG2bY+JVl2qTJOA8uxl8OkL8PZd0ecNY08IVkE4SHUgnDi9uUkFZWdFPT5CFLCbQCt3O/Y9ehrP+37PMGtzNEBEmlQ2OrCC4ly7ZXuSgPLix0XU1DrH1bRcQdmXq//RfNBwZ3Lm32Ak+fRyke7qzRU7+c2rKwnp/41uS9OMHRoDyskDc/jh2UMYd2J+u8+RbAxKD0d1Yl9dKW6XhdflYrF9fGyb34qOxagwGdGBMU27eHYsjf63sYICULeXmkCY/o4uHldVYkAprqjj7977Odu9iullzwJH0YwjtY10raeyLgSRJkvNd2QFpSGg9GUvO/Y2H19SFQiThuP9WxmDsi+rdlRS7/eS1nB9CdaAL3O/z9dezr8pgbCN36Pp0yIQrShe81z099oZR/XmouH9u7hF0hVUQUkiN8PHJacMIM3b/i+MZmNQPIljULLTW8+EHpeF223FB8QCm+xoUKqMVVDKE14Trt4TfeAcBFtXRnUgEruXD4A7UJEwZmNnRT1nu1cBcPz2l5I3yBEAvIQpqwlCKNDQnobxM5FA0hlC+6M6EOEM12oWp02icO19zfb73C7SnIvEHUBAATDOmFC28YDO1V7O2yMFQvpXokij4sp4lVbVxe5LAcWhI0YBOLt4fB4XLpdFmid+mXu0MtYDohWUxnOcH7ifO0Pf5zX7TAD2NIxFoXZPwmtqK3ZTH4okBpTaPeypCSTO4gGo2RV76Pwl4Gqpq8TxXtnUsqcmiGVHKxh7TRamccpvG6so4YjNn99ez5Ityaf11gTC3OyZDsAXyl9ptt/tsjqsguIjRLrlOFcnBxTnL95AuGvHwIikkk+K4lXicETjs7orBRSHxoGKB7I+hdsRUBqDid9RifG4W7/kHpcLT0MVZr0ZyLTIhYQbeuJ22Q0zgKp3JVQssq1aFm/andDF88+5y1i2tTxhFg8ANbtjD0vL42M53OEWvuhr48fnWWWU1QQwDXcTDuAl7G1oUxvHofz74208+PY6Ln90ftL9NcFwq3NbaoMR0pyh4gDGoGSR+Nra3Vv3+1z7IxCKkEkdF7sWEKxrw/ovneCemav51uPzCYb1r1bpOiu2l8ceV7W03IAc9hRQkjiQyZ/OCkpjF5HfUUFpOogWwOsYt+JxW83GsTQqbgwogQrmf7oqYV/99tVgx/9Hdm+cy8PeqZzk2pR4EkcFJVAZf+xtKaDUxCsoeVY5ZTUhXA0VlCBeQr6GNrWxgrKupPUbC1YHwliOWtacTzcnTD2uDoTxJ1RQ9v+LPctKDChLV65q4ciDIxixuc/7JH/1TSVn/v37fZ6K2hCzVu7skCrM3z7YxKJNZcxbt2vfB4scJKt2xBeOrOrAO7HLoUUBxaEjConOCkq6LxpQfI6A4k1SQUlzDI50uyyy0xK7gc44KnozwkoyCZrosW++817CMf4dCxOe/z/PXL7iXkCeVQ7A7oaZQM6AEqmJd7PkhEoJLH+Jux95gr+97+jqcHTxnOdexnmb/4inYaG2IB7q3I0BpW2LtTkDWrKptTWBcEJl43f/ms1tr6xM2J/YxbP/FZTsJhUUT3XzhewOFts2hCKGr7ij93HqtfKp/T7Xj5/9mJ88u5S/zGllvZc2cM6WUAVFulJZTfz/8Q69E7scUhRQnBq+L9u1/kkTzgDSGDz8TQJIUwldQC6LPj38seffGj2QaT84nUyfG7DYQ/QGet69iV9G2WWftNgmGxfz7WHRJ46A4nIs754X3oH/lSu5tvQO7v3viviLHV08AGMrZzDcFQ0wQbwsLmsYKFtRRChiM/n5pUz7sEnVhuiXX8Q2CZ+/sq75L56a+jADrPh7DrB2869FWx37A/gsR6WgjV08ycJQdpMKSm6486oGwYiNm/jnCKX1adfr60MR7IbK0oKN0T/H6YuLWnvJPlXWxStwttZlkS5U4fi7WKkKSrelgOLQuJLsgXTxJIxB8TaMQfEkXuaZkTEAvBT5YrP97iYBxet2keZ1M+u6L/LhLeeS138QAMdYiUvT51etpCVlaYPZbvpGnzSMQTHG4A40H6iabdVxgrU19uXXdEAuwAT3HACCxsNmk9fwJhv536piZn66kztfX51wvDGGK55YwNn3zWWP419Gu2sCNJUW2B2f9guxsNLYNRSqbxJIWujiCUdssqjlm+53SSNAoElFwBhDNtFurbCJXv+ekd1NT3PQBEI2x1rbYs8jLl+bX1tWE2TM7+dw9T+XJGw/0Hv7OL8U9K9W6UqNfxev97zE1zbfA7Yqet2RAkoSrf2i39eXQLIxKCcWJC5vf3Pox/w4eB23hX4AQL/seCCxLIs+WYkBBWBQrwwG5Kbj6tEPgGNciWua5AejVYZaK1rR+NyOrxsQ9OU06+KpC0XIspPfIHCUax17axuCRE3zL+3eDTODgnjY0hBQzJ6N7K2Nf8E5uwjWl1azZMtedlbU8/HmeCjaUx0PK+GITV0wQq+QYy0X4gFl0abo65oFlCTtA6gNRfiF5wX+4H2Cx70PJiwAB9F1RxorKOtMNPT1iuxuvlrfvmyZD/+4NHrTwXYIRCKMcMW70vy1JW3+Jfzq8u1U1IV4e01JwvYDXTjfGVCcj0U6k20bKutCDLW2ca3nZQorZ0FxyxViOXxpoTaHtnw3WbQ+VsWdJKD07uHng1+eQ0bDkve1pPE/+/TYcUf37cHFJ/WnZ2b0X9F9esT/Nd1sUG1jQLGS39zvufB57LKzCBxzMXdt+R4AgYz+7NmdGFDKaoLNZ/g0GOVaR2lVgN49/LFl86eEvsOb9um8578+dlwQL5tNdI2WQOl6gkfGv2B3VwcoyI2GpXlr410n5XtK+I/vAd6InM6e6pGx7Vc8sYDPiqs41yR2sxxvFZFNDcUV9fzhf5/xyaZiSHMcULGNZGoCYb7hjo7T+ZL7U7bWBaOfp3F/eSm9Gj7/BlPAMLbgJRytGGW2o7vl6Quj/33pSrjmgza/LBCyOcrxZ+gyIagphax9Lw5Y67gdQNPgdSCcoaS8th0BxRioKILsgeDSv3nkwFQHw9gGvu95K75x1zooOLXrGiVdQr9NHOLBo+V/i55YEB0D0rTbppEnSRcPwMCeGfTKTF7Gt4CrvngU3xg1ECChi6eu6RL0DV+ejUvYFzV23TTYYvflRf9l3P79r1A8dipm4GmsGX4ze2gMKNGKw96aUOIy+A4jXesprWrofqkpjZ7X5LHV5LHF7hc7rspksLlhETlf5RZOXvE7fut5mr6UU+JYY+W9daVc6/4Pd3me5v+55zDKtZ7bvc9RVhV9/8r6EB9v2Ut1IMxAKxpQtlrRCtD57iV8mnYVwU0LeOSdz0m3mnQLtRhQInxu4rcqMJvmxXcWLabXoyO42fsCEL2VwC4T/XM15fs51bhkxb6PcQiEbQZZTca8tPBZmr2V49pu3hOffdWRXTztqqDMfxgeGg7LnzuwBjQIhCMt38m6LYyBF/8Pnv0GRNRVdahpHAt1jnt5fOOuNV3TGOlSCihJtPaL/tHvjuSbowby6uSzku53riSb3saVaC8bOTDhuXMF22ZfFJn9Ep5u8yYuT19iejKsIBuP20X+2ROxfvQ2ntwB7Gn4AqZqJxhDVdlOrvK80fCa3IRzDLD2UFmyKfqLvqHrYlNDpWS1OSJ23If2SRTTk4Dx4sJmdMmLfN8zm+m+u9lVEa1OGGPov+0Nrvf+h4me2VzoXhz/KNuiFYfPS+NBqTGgvOMqTGjTyTujYaJxFdl603DLgNrdSVexrQ2G6W3Fu7D6fXQnhBvCzauTYovNAWw0/VlpHwlA4LPZzc6VlDFQ+lnbjk0iGLYZbJUmbqxoPsh1b02QqXPWJ8xq2Lwn3s21Zme8ClZ/gKvRJgxMbGtAiYThrduij9+dckDv3+ibj83ngnvfoLIi+WJ++1SzC1bNgA2zYcuHAGzdU8tNL36S9AaUkloq6kJ4CdOf+Pi33Zs+7cIWSVdRQHFoSxfPwJ4Z/OGbJ3N8fnbS/c41TFpaKr+xQjK4Vwbv/OLLFB7du8X3axZQhnwx4enSzLMTnpeYnhzTt0fCth5pHjaafOrwQ+1uwlsW4JtzR2z/B/ZJsccRE22/b8fHUL4FApWE8MaqEdWNy9sDI875Brd95SSW2kMT3u9o1056rZyGbRu2vfMUvzd/ib/GsS7LwOK3qagN8eyCeNWiMaBsMon33uhp72W09Rlfcn0S+5w1pqHSVNm8u6umLkge8anP6eXr2TDveeYvWQK7E8eLfGwfx6yGLjffR39sdlPFZkL18Py34K9jEre3cfzKjvI6Lp76PoMaAsqKhnBEyepmx/7k2SX8afY6Rt49mwmPzmPPrp1s3h2vmny2s5ILXYt4y3cTR9WvPqAbq1XUhsikjn957+GarTeAve91VbYuejX+JLNvywe2UWV9iLXbdvGifSOeJ78UD5Xt4VgReN7r0wD4/t8X8uKSbVz9z669QaTsW0VdiAJrN24r/v9TVdHKDu3OlEODAopDR8ziOWVQbuxxS9OVX/jxGXx79CD+eeXpDOnT+s3pmv1Ltv8IOPk7AMyNnML/POdQb8VDQ4npyTH9EgNKlt9LHWm8EYl+CS9/cQonlr8DwJysS3gs/LXYsY3TkYMrX2Xjiuhqr1s9RxDGwzH9evCvyLkALHCP4kfnn8qZR/fmlvCPqCSTOvw8FL4MgBM2Ps1f/vcJafN+i8dK/qU5bO9cbv7XfN5YuoFBVnTAZ+Og2K12H24I/iR27Jnu1Uz33RPrlqny9WGnaQh2T54LO5YnnDtcVYzHsgkbF38LXwTAsndepvSV25q1Y60ZxOzIKMLGhSsSoP7hM6NL6G9fGg0jAPP/Cv+eCIFqPp75BKx/q9l5qN33v/iNMVw7fRknWFvIaRikOyPyBQDWL57F7opqmHEN/O/XYAwLN8XPec2OW+nx6Kn0rljBk94HuN7zIp8VV/EH7+Mc69rODP9vKK9q+7owwbDNk+9t5PNd0QpWdU01D3kfodC9mpMCy7jv3tu59z8N42pK17DqmWv55aMvsNdRzVn9/oz4CSsTg11RWS3n/vFd/vz2+ja3aUNpNSdbnzPItYuM6q2w+f02vzbGEVCOKZtHeU2A4j17Ge9awLrtewhHbKa8uYbfvr66+fTzNTOj44ka/iyTTU+Xg6uyLhSrLjbe3uMIq5SdS17vymZJF+jSgPLII49w5JFHkpaWxpgxY1i0aFFXNifmQPryzzgqXg1ZvTP5LJmj+/bgvm+M4Ije+75zbm5GknEr4//IzaGruDY0mZpghBUn3gTAXtOD3eRwdL/mFRSA6eFzABhdM490K8gGu4B/9pwU674B+HfkywB81b2AvHduAGCD60ggOhtpqTmWsYH7uS/rFgCO6pvJNqs/59ffxwWBe3k4fCnFpieZoTKuXfgl+loV7DbZTLDvir1HlUmnyPQlw9TyeNFXWZP2Q973X88Lvt9ytCs6i2dLpA8v21/k6Pp/UttQKXEGnc1HXkGAhm6e+nLME18m+Pn7ULyCkqUzyVn1TwBK6Mk8+2QAvul5j0vcH2Ebi3WuaNdYpcnAxkUZ2dwW/iEAaaFy6p68CJ48B/51BSx/Hv53K6x+hZ0v30LfZfGKkNO6Gb/j1SfuZNl/HgDbxtgRNs5/haVv/p1gXTVm/du88thtnFr0D9703xp73Vz7FACG1n3KO0/eDJ88D/MfpvKjpwBDGgEK2M3Z7lX47TpmeG/nfPdSrvXMoN+OOWRZ8W4L/2s/hkiS7pnyIlj2bEJ32L8WbeV3b6zml3/+O2uLSjjtk9s43700tv+XwUf4+ic/obS8BvvVyZy4aRr3lVzN+n9MJrJuNuxay0n1jqnONbsgUBVdfbiqmIfnbmDjrhoefHsd98/6jFF3z+adNSXRxfWa3pG7weel1Yx2xStctSv/2+yYPdWBhHE4zTgCygBrDxs//ZDbPM/yiG8qv/I8x69nrOTpeWt57sO1CV1kL31cBC9MgJUvwSvXsHX9p5x171wmP7802bscsH9/XMTzCzv39grJVAfC/OSfSxLWG+pKFY6AstQeygvhL+OyDAXzbtZ0426my2bxvPDCC9xwww089thjjBkzhoceeohx48axdu1a+vXrt+8THAQXnpjP8AE5ZKd59/scbpfFz889hqlzN/Cjs4fs93me+9EYHn9vI3dfclLznb5M/h2Jho2sYITBF0xi7JIeRHATwd2sgtJ4g8LF5njmRE7lPPcyAB6PfIVhBTm8u243Zwf+jJsI3zr/izw0dyc/db9KZsNKq+/XRKfhjhiYy6vLd7DBDKR3VTTF+T1uRgzMYdlWExtlPC08jlu802Pv/2rkLMIDRvHRjmHkW2U86foG5wwwDNr+SEI7x7jiYzpOH3ESny/ZyYjBvXmvdCQXEr93zzp7AF+45CqWPfgG2FsAsDD4/vkVAPIafiC6Vssi+3gCxou/YX2VGfZZ3BH4Add6XubFyJdi550eOZdM6rjd+xzpuxqmNW58J/rToP/af7YY64/d8BTHAuyA7cXzqKwLckJ1dKXY4KJbsUw9Xwe+7vjrtcYezGaTT7HpSb61l29WxweaZs++kbX+eLuT+ZMdv+Nz0LjJ3jgT3riJmi/dwZyPV5NNFSODH5O94IHoQa9OIjhgDPUnXM6KD+t42vsy57g/Ydvf/sxYaze2sZhrn8LYhr8jx7mKWPbyr+i3Pd41cnrJC/B8tJKVOHoKnn/lNb6y4Q4y7FqI/IjLXEHGupfy4ntf4seu1ZzzQkPg8KTDxffDqd+L/otgzUxY8Ffc3vF8zR3/sw4sf4mlR17FiUOPoWeai9kvTGXtmk/5H2fx4M+u4Jh+Wc2uSWT357iJdle6LcPI/32dkQ2/6f7P8xZnLxnPf333k2XVMm/NdIb5+2Kn9eRv/5nJNxrHp6+bRf66t/EE/sCsT6t5YmAOD85ez+8vO4mvn9r0UycKhCMJizMC0VtBrHiRyiPHsWve38ivXM4D67/FWPdSdnA2BWOilcdwxGbznpqEz1VcXkeuJ0BaZs6Bj4R2aFw48R/zNzNrVTGzVhVz2cgBzdveDm+s2Mkv//Mpv73kRM4+pi99HUsmtFVFXSjW/Vlk+nFf+Aq+6p5PRqAUSldDfpLfiXJYskwX1TDHjBnDaaedxsMPPwyAbdsMGjSIn/3sZ9xyyy2tvrayspKcnBwqKirIzk4+FqQrGWMoqQyQl+0/oFVpW/O9pxby/vrd3HD+sfz8vKHcPXM1T32wiZ4ZXpbefn7C+4YiNmf8fg57aoLks4cpmf/ilHO/xbP1Z/O9wiO49JEP2bynlh+eNYQ7vjqM3dUB7puxkPo1/6O/tYdpkQvJyMhg7o1f5if/XMKizWWcPyyPJ78/GoAte2r47lMLKSqrw+u2MJEQl7vfJ58ycqwapoa/zs1fP5P7//cZ5bUhvnZyAfdfdiL/fvF5vvz5/Qw221k//BesWv4R41wf4znuAgKXP8Nry3dwwYl5/Ohv7zF29z/JpJ5HwpdSi5/V917OhpWL2PDy3Uytu5DbPM9ypjs6hqPO+GJ3KV6acRY/Dd/IpTX/5heef1OLn4sC97KdxPES44f3578rdpJFLS/7fsPQJuvMvBs5mdNdn5HRMItonX8462vTGe9eRK3xs9g+jpGu9QnVjJbUubNZZR1DScDDfyJfZGVmIRfWvsZN3pfIpJZP7aNYYw/m6+4PEhatg2jFp3H9lnKTSa4VHY9yd2gC20w/Hvc92Oa/Q8m8GTmNP4W/yWz/zc32vRcZzgf2SZzj+oQTXFti7w2wzD6GU13tX2p/q2sABXYJHloeX7DLZLPCOo5hrq3k2/G1X9Z4jicrKwdyBmL7suixezlVPY6i5453yQ6XMTMyhq+4F7Z4XicbFy6S/+vcNhbz7BGUkc3p1mfkDDuXuowCPvcdS9/di7AiITJ9Fj2p4sPdGRTt2suIwX3x9BzIgup8Tgl/wsid0/G0cL+rCC5e7n8jA3tn8smW3bB3M2kjv80VJ/XAfv16PJXb8FlhggPPZNdpv+DFtSFKi3cyJNfijNML8UZqcRNhQPVKfHnHss13FOnlnzN/aw27Pfl8b5gresPRjN6sKa5mx2cL+e+GIOvrsnhw4pd5cM56tq1bxlXuNzj+nO9w8nlXUFEbYsfOHRxtb8SXnccGaxCBUIRj87IorQ6SneYhK81LbTCMq+F3zSvLtnPvyx9xmfsD+lt7mG8P4zvjz+f8s85gfUkVv3r5U8Yf5eH/zh8NrpZD0AP/W8uoD67iHPcnvNr/51y76Qymee/jy+5PMBfcg3Xmz9r0Z3qwGWOoDUbI3Mcd6iVRe76/uySgBINBMjIyeOmll7j00ktj2ydOnEh5eTmvvvpqyy8m9QNKZ6iqD7F4cxlnH9MXn8eFbRueW7SVo/tkcuYxzdfxKCqLfql9vquao/v2YFCvjNi+1Tsq2ba3lvOH5cWCTWlVPc/O30JlfZjsNA/fLTyCfllphCM2b68p4eRBufTPiY99idiGksp68rLT2Lynhj6ZfqoCIWwb5m/czTdHDWJ7eR1V9WFO6J8VD1DhAJRtgn7Hs3hzGZaxGT0ksf0LNu5hxtLtHN8/i4E9M+jdw8fIwT2B6KDKz3ZW4cLm0/dfY3dNhOxh5zLY7CRv+V/odc4kTMFIHn33c7JCu+mZ4aU+vR/BsM0ZR/Xm2LwsZn66k++eMZh31+5iT3WAfhkW1Sv+y+LQkVRUVTHatZ7NeRfQp2oN2cESTvjStxiS15NfzfiEc2rf4vXQGKo9udx2/mBO7u/nzn/M4od7/8xA1x5KTvwRszznUbz4ZXaZHDb2/AIv/fRsdlUHuOThD7nqC0P4xqhBvLlyJxMLjyASifD4+5t5/L2N5FLFLcPK6J3uov6T/9DPW89dvht4eKwflz+Li58r4WLfcs44dQQ9hn6BX7+ykjOq3uJ277P0tKqpw08YN2vtgRi3n2esSwjW13CUtZNL3B+SST09PfVkFv6Iuso9FH3yDpNDP2ODGcjXXB9yw9dOZ+sH0/li9ZtUmXR+6f4Fy7ynUl0f5rQje7Jr3UIe8j7CvKzx9Ins4mv1if/f1pKGq+9Q0nYlTsFeaB/POnsg33HPbTY+KWIs9pLFS5Ev8WLkizzv+13sflIQHZOwyXccpwaXJAyibMo2Fr8Z+DdO2fo0WdRRj4/37eHc5nk2NvanJc+Hz2WgtYsvuts3dfxQVGd81OKPLb4IsIN++E09OVTH/nxKTS451OAlzA56k0k9xnJTbzykESRg+am1vQy0djer+G1xDcbYIXqYGvpYlVRZPahw96LO9lBnvLg8XizLwhgLLIvq+iBnuNZgsLB+PI+V9pG88tdbuc0brS6uyv4C1e5cjOUCLCIGbBOdoGBsQ3qoDMvlBrefsMtHxPJgGbvhJqSGaL3VwlgurIZnjUKRCC7AclkEQxE8botg2MYYQ6bPjQE8kXoK6tbjDVUQsqEkYyi2y0cEd/R9XR5cLhdY0fdpSbJvXpPwONpGu6Fk23guy3K81rKiWy2wsKLjKA24TASXCeMykeg5GwrcJvYP13jbnMtrxLdF/+sacjZjLomPBewIKR9QduzYwYABA/joo48oLIxPJ7355puZN28eCxcm/qsnEAgQCMRH81dWVjJo0KBuHVDk0FIXjOB1W3gc92oKR+yE506hiI0xiTeabCoQjuCyrNhqwxHbUFpVjx0O4Y7U0bd3X8LGsGxrOacMysWyovc/qg9FsKzo/Xz6ZafFujS37qll5Y4KBvXMIGIMpwzKJRi2eWfFRipDHs4/qYAefg+BsE26180ry7ezekcll546gEGZhtVL3mXU4Fx8/nTq+w7H5XLh86dRUlHHa2/P5esj+lGcPpTFW/ZSXR+mT2g7J4dXsrrKz56aCL5jzyGCh+x0HyOP6EkoYvPqx5/zZfdKvLXF7Kz3stB7Btd/ZSR7t33G7g+epq6qAlNbRtDyscs/iMxIFaXhDEz+CCZOmMiUN9fwSVE5Zw/tw/cLj+SZ197i2OKZnDfyeB7cMoQTi6azK/sEKtIHMy74Nlknf5UPPIUs2rSHcypfY9ywPjywti8XB96kP3tYsQcqyKTAKuMEazNh28JDmNUMYUu4N8M9W9jqGUK98TCczxng2s0mBrIu2Jul4SM5yV1ErzMmsHvHJoaWzaMolMWg+vVkWnXUWRnkUUaa29DXLiViXPzXHsOrvq9yJNv5afif2Fj0dtUQ9mZTGTTkmT2E8OC3QpSZHvgI08OqZ5fJIceqwUeYcpOJC5vshupeKb3xmCC9HKGkJTtML3pTid9q+wyaIu+RRHofh3/PavoGt7c4SH5fNhw9kWO+NxVjDI+98g7/b/l3ybGSV6Hk4FjY+xLG/OwfHXrOwy6g3Hnnndx1111NT6OAIiIpwxiDMeBKckPQRk1vmAktjFmxI5TX1GNcXnpm+jDGsLc2hMuKD5yvqg8RCoXplekHE2FndZhwKMwAfz2uHn0gWEOwrpp1Nen07uEj02PTgwCuzOjd0SPBOj7/fB11tdUcc9RQMnL7UrZtHXt3biTo6cFx/XtSlT2Ujdt3MiC8jdze/VhTGuAY727qvbls21tL/0yLsnpDRUU5Q/tm8HmVixNGjCErI7rc89oN66javJyszEyGDMyn2MpnV9Fa6msqyfaE8Zgg9fUBQpEIbhcY21AfDJPfpydHn/UNLHe8+2RP2R6WvTcTT80O0oN7G8oIBpdl4bIgHDG4XBBJ600oYmOH6vHYgWgVwXI1VA8aqhAmgmVMrHpgGuooPo8Lg0XENqR53YQijee3qA/buCywXC72Zh2LnVVAf1891TvW4nMbXMbGjoSwwyHCkcZqTZzVwpOmf1saKzouYwM2ljEJ3Y/G8RpD9DJEqySmoaJiMJYH4/JEq0wNVZam1aLG9jVus0zzNqcPHsnwc75JR0r5gNLeLh5VUERERA597QkoXTLN2OfzMWrUKObMmRPbZts2c+bMSaioNPL7/WRnZyf8iIiIyOGry4Yf33DDDUycOJHRo0dz+umn89BDD1FTU8MPfvCDrmqSiIiIpIguCyjf/va32bVrF3fccQfFxcWccsopzJo1i7y8vH2/WERERA5rXbYOyoHQNGMREZFDT8qPQRERERFpjQKKiIiIpBwFFBEREUk5CigiIiKSchRQREREJOUooIiIiEjKUUARERGRlKOAIiIiIilHAUVERERSTpctdX8gGhe/rays7OKWiIiISFs1fm+3ZRH7QzKgVFVVATBo0KAubomIiIi0V1VVFTk5Oa0ec0jei8e2bXbs2EFWVhaWZXXouSsrKxk0aBBFRUW6z89Bomt88OkaH3y6xgefrvHB19nX2BhDVVUVBQUFuFytjzI5JCsoLpeLgQMHHtT3yM7O1v8QB5mu8cGna3zw6RoffLrGB19nXuN9VU4aaZCsiIiIpBwFFBEREUk5CihN+P1+fvOb3+D3+7u6KYctXeODT9f44NM1Pvh0jQ++VL7Gh+QgWRERETm8qYIiIiIiKUcBRURERFKOAoqIiIikHAUUERERSTkKKA6PPPIIRx55JGlpaYwZM4ZFixZ1dZMOGe+99x5f/epXKSgowLIsXnnllYT9xhjuuOMO+vfvT3p6OmPHjmX9+vUJx5SVlTFhwgSys7PJzc3lyiuvpLq6uhM/RWqbMmUKp512GllZWfTr149LL72UtWvXJhxTX1/PpEmT6N27Nz169ODyyy+npKQk4ZitW7cyfvx4MjIy6NevHzfddBPhcLgzP0rKevTRRxkxYkRs0arCwkLefPPN2H5d34537733YlkW1113XWybrvOBufPOO7EsK+Hn+OOPj+0/ZK6vEWOMMdOnTzc+n8/8/e9/N6tWrTJXXXWVyc3NNSUlJV3dtEPCG2+8YX7961+bl19+2QBmxowZCfvvvfdek5OTY1555RXzySefmK997WtmyJAhpq6uLnbMhRdeaE4++WSzYMEC8/7775tjjjnGfOc73+nkT5K6xo0bZ55++mmzcuVKs3z5cnPxxRebwYMHm+rq6tgxP/nJT8ygQYPMnDlzzMcff2zOOOMMc+aZZ8b2h8Nhc9JJJ5mxY8eaZcuWmTfeeMP06dPH3HrrrV3xkVLOa6+9Zv773/+adevWmbVr15pf/epXxuv1mpUrVxpjdH072qJFi8yRRx5pRowYYa699trYdl3nA/Ob3/zGnHjiiWbnzp2xn127dsX2HyrXVwGlwemnn24mTZoUex6JRExBQYGZMmVKF7bq0NQ0oNi2bfLz880f/vCH2Lby8nLj9/vNv/71L2OMMatXrzaAWbx4ceyYN99801iWZbZv395pbT+UlJaWGsDMmzfPGBO9pl6v17z44ouxY9asWWMAM3/+fGNMNEi6XC5TXFwcO+bRRx812dnZJhAIdO4HOET07NnT/O1vf9P17WBVVVVm6NChZvbs2eZLX/pSLKDoOh+43/zmN+bkk09Ouu9Qur7q4gGCwSBLlixh7NixsW0ul4uxY8cyf/78LmzZ4WHTpk0UFxcnXN+cnBzGjBkTu77z588nNzeX0aNHx44ZO3YsLpeLhQsXdnqbDwUVFRUA9OrVC4AlS5YQCoUSrvPxxx/P4MGDE67z8OHDycvLix0zbtw4KisrWbVqVSe2PvVFIhGmT59OTU0NhYWFur4dbNKkSYwfPz7heoL+HneU9evXU1BQwFFHHcWECRPYunUrcGhd30PyZoEdbffu3UQikYQ/DIC8vDw+++yzLmrV4aO4uBgg6fVt3FdcXEy/fv0S9ns8Hnr16hU7RuJs2+a6667jrLPO4qSTTgKi19Dn85Gbm5twbNPrnOzPoXGfwIoVKygsLKS+vp4ePXowY8YMhg0bxvLly3V9O8j06dNZunQpixcvbrZPf48P3JgxY5g2bRrHHXccO3fu5K677uILX/gCK1euPKSurwKKyCFo0qRJrFy5kg8++KCrm3LYOe6441i+fDkVFRW89NJLTJw4kXnz5nV1sw4bRUVFXHvttcyePZu0tLSubs5h6aKLLoo9HjFiBGPGjOGII47g3//+N+np6V3YsvZRFw/Qp08f3G53s1HMJSUl5Ofnd1GrDh+N17C165ufn09paWnC/nA4TFlZmf4Mmpg8eTIzZ87knXfeYeDAgbHt+fn5BINBysvLE45vep2T/Tk07hPw+Xwcc8wxjBo1iilTpnDyySfz5z//Wde3gyxZsoTS0lJGjhyJx+PB4/Ewb948pk6disfjIS8vT9e5g+Xm5nLssceyYcOGQ+rvsQIK0V9Io0aNYs6cObFttm0zZ84cCgsLu7Blh4chQ4aQn5+fcH0rKytZuHBh7PoWFhZSXl7OkiVLYsfMnTsX27YZM2ZMp7c5FRljmDx5MjNmzGDu3LkMGTIkYf+oUaPwer0J13nt2rVs3bo14TqvWLEiIQzOnj2b7Oxshg0b1jkf5BBj2zaBQEDXt4Ocd955rFixguXLl8d+Ro8ezYQJE2KPdZ07VnV1NZ9//jn9+/c/tP4ed9pw3BQ3ffp04/f7zbRp08zq1avN1VdfbXJzcxNGMUvLqqqqzLJly8yyZcsMYP70pz+ZZcuWmS1bthhjotOMc3Nzzauvvmo+/fRTc8kllySdZnzqqaeahQsXmg8++MAMHTpU04wdrrnmGpOTk2PefffdhOmDtbW1sWN+8pOfmMGDB5u5c+eajz/+2BQWFprCwsLY/sbpgxdccIFZvny5mTVrlunbt6+mZza45ZZbzLx588ymTZvMp59+am655RZjWZZ56623jDG6vgeLcxaPMbrOB+rGG2807777rtm0aZP58MMPzdixY02fPn1MaWmpMebQub4KKA5/+ctfzODBg43P5zOnn366WbBgQVc36ZDxzjvvGKDZz8SJE40x0anGt99+u8nLyzN+v9+cd955Zu3atQnn2LNnj/nOd75jevToYbKzs80PfvADU1VV1QWfJjUlu76Aefrpp2PH1NXVmZ/+9KemZ8+eJiMjw3z96183O3fuTDjP5s2bzUUXXWTS09NNnz59zI033mhCoVAnf5rU9MMf/tAcccQRxufzmb59+5rzzjsvFk6M0fU9WJoGFF3nA/Ptb3/b9O/f3/h8PjNgwADz7W9/22zYsCG2/1C5vpYxxnRevUZERERk3zQGRURERFKOAoqIiIikHAUUERERSTkKKCIiIpJyFFBEREQk5SigiIiISMpRQBEREZGUo4AiIiIiKUcBRURERFKOAoqIiIikHAUUERERSTkKKCIiIpJy/j+BMXN2DZGjzQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(v.history['val_loss'][5:])\n",
    "plt.plot(v.history['loss'][5:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
